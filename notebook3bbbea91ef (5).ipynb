{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7625342,"sourceType":"datasetVersion","datasetId":4442249},{"sourceId":7625566,"sourceType":"datasetVersion","datasetId":4442422},{"sourceId":7639623,"sourceType":"datasetVersion","datasetId":4452417},{"sourceId":7641569,"sourceType":"datasetVersion","datasetId":4453704},{"sourceId":7644544,"sourceType":"datasetVersion","datasetId":4455863},{"sourceId":7649539,"sourceType":"datasetVersion","datasetId":4459255}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport tensorflow as tf\nfrom PIL import Image, ImageFile\nimport os\nimport random\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision.transforms.functional as TF\nimport random\nimport cv2\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-19T06:44:30.665622Z","iopub.execute_input":"2024-02-19T06:44:30.666815Z","iopub.status.idle":"2024-02-19T06:44:49.459446Z","shell.execute_reply.started":"2024-02-19T06:44:30.666768Z","shell.execute_reply":"2024-02-19T06:44:49.458654Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-19 06:44:39.124932: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-19 06:44:39.125027: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-19 06:44:39.258610: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim\n\ndef calculate_psnr(target, prediction, max_pixel=1.0):\n    mse = torch.mean((target - prediction) ** 2)\n    if mse == 0:\n        return float('inf')\n    return 20 * torch.log10(max_pixel / torch.sqrt(mse))\n\ndef calculate_ssim(target, prediction, data_range=1.0, channel_axis=-1):\n    # Convert tensors to numpy arrays\n    target_np = target.cpu().detach().numpy()\n    prediction_np = prediction.cpu().detach().numpy()\n    # Calculate SSIM over the batch\n    ssim_val = np.mean([ssim(t, p, data_range=data_range, channel_axis=channel_axis) for t, p in zip(target_np, prediction_np)])\n    return ssim_val","metadata":{"execution":{"iopub.status.busy":"2024-02-19T06:44:49.460926Z","iopub.execute_input":"2024-02-19T06:44:49.461467Z","iopub.status.idle":"2024-02-19T06:44:50.141151Z","shell.execute_reply.started":"2024-02-19T06:44:49.461442Z","shell.execute_reply":"2024-02-19T06:44:50.140146Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif device:\n    print('Model is running on GPU:', device)\nelse:\n    print('Model is running on CPU')\n\nclass InpaintingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.image_list = os.listdir(image_dir)\n        self.mask_list = os.listdir(mask_dir)\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n\n        # Ensure the lists are sorted so they correspond\n        self.image_list.sort()\n        self.mask_list.sort()\n\n    def __len__(self):\n        return len(self.image_list)\n    \n    def dilate_mask(self, mask, dilation_kernel_size=3):\n        kernel = np.ones((dilation_kernel_size, dilation_kernel_size), np.uint8)\n        dilated_mask = cv2.dilate(mask.numpy(), kernel, iterations=1)\n        return torch.from_numpy(dilated_mask)\n\n    def create_weight_map(self, mask, dilated_mask, border_weight=2.0):\n        border = dilated_mask - mask\n        weight_map = torch.ones_like(mask)\n        weight_map[border == 1] = border_weight\n        return weight_map  # Add this line\n\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(self.image_dir, self.image_list[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n\n        image = Image.open(image_path).convert('RGB')\n        mask = Image.open(mask_path).convert('1')\n\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n\n        # Ensure mask is a binary tensor with the same size as image in the channel dimension\n        mask = mask.expand_as(image)\n\n        masked_image = image * (1 - mask)\n        \n        mask = (mask > 0).float()\n\n        masked_image = image * (1 - mask)\n\n        dilated_mask = self.dilate_mask(mask)\n        weight_map = self.create_weight_map(mask, dilated_mask)\n        \n        \n        \n        weighted_masked_image = masked_image * weight_map\n\n        return {\n            'ground_truth': image, \n            'weighted_masked_image': weighted_masked_image, \n            'mask': dilated_mask\n            }\n\n\n\nimage_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    \n])\n\n# Define your mask transformations including random rotation, flip, and dilation\nmask_transform = transforms.Compose([\n    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n])\n\n# Create dataset instances\ntrain_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/trainingimage/new_subdataset',\n    mask_dir='/kaggle/input/mask-dataset/training',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n)\n\nval_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/valimages/ValPlaces2',\n    mask_dir='/kaggle/input/validation-mask',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n)\n\nfrom PIL import Image\nimport os\nimport math\nfrom torch.utils.data import DataLoader, random_split\n\ntotal_size = len(train_dataset)\n\n\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T06:44:50.142787Z","iopub.execute_input":"2024-02-19T06:44:50.143516Z","iopub.status.idle":"2024-02-19T06:44:51.854855Z","shell.execute_reply.started":"2024-02-19T06:44:50.143482Z","shell.execute_reply":"2024-02-19T06:44:51.853785Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Model is running on GPU: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.models import vgg16, inception_v3, VGG16_Weights, Inception_V3_Weights\n\nweights_path = '/kaggle/input/vgg16model/vgg16-397923af.pth'\n\nmodel = vgg16()\nmodel.load_state_dict(torch.load(weights_path))\nmodel.eval()\n\nclass VGG16FeatureExtractor(nn.Module):\n    def __init__(self, weights_path):\n        super(VGG16FeatureExtractor, self).__init__()\n        # Initialize VGG16 model\n        vgg16_model = vgg16()\n        # Load the pretrained weights manually\n        vgg16_model.load_state_dict(torch.load(weights_path))\n        # Extract the features portion of VGG16\n        self.features = vgg16_model.features[:23]  # Adjust based on the layers you need\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        return self.features(x)\n\n# Path to the manually downloaded weights\nweights_path = '/kaggle/input/vgg16model/vgg16-397923af.pth'\n\n# Correct instantiation of VGG16FeatureExtractor\nvgg16_feature_extractor = VGG16FeatureExtractor(weights_path=weights_path).to(device)\n\n# Updated ContentStyleLoss class initialization to accept weights_path\nclass ContentStyleLoss(nn.Module):\n    def __init__(self, weights_path):\n        super(ContentStyleLoss, self).__init__()\n        self.feature_extractor = VGG16FeatureExtractor(weights_path=weights_path)\n\n    def compute_gram_matrix(self, input):\n        a, b, c, d = input.size()  # a=batch size (=1 for simplicity)\n        features = input.view(a * b, c * d)  # resize F_XL into \\hat F_XL\n        G = torch.mm(features, features.t())  # compute the gram product\n        return G.div(a * b * c * d)\n\n    def forward(self, generated, target):\n        gen_features = self.feature_extractor(generated)\n        target_features = self.feature_extractor(target)\n        content_loss = F.mse_loss(gen_features, target_features)\n\n        # Compute style loss\n        gen_gram = self.compute_gram_matrix(gen_features)\n        target_gram = self.compute_gram_matrix(target_features)\n        style_loss = F.mse_loss(gen_gram, target_gram)\n\n        return content_loss, style_loss\n\n# Correct instantiation of ContentStyleLoss with weights_path\ncontent_style_loss = ContentStyleLoss(weights_path=weights_path).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T06:44:51.857120Z","iopub.execute_input":"2024-02-19T06:44:51.857434Z","iopub.status.idle":"2024-02-19T06:45:05.318261Z","shell.execute_reply.started":"2024-02-19T06:44:51.857408Z","shell.execute_reply":"2024-02-19T06:45:05.317453Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nclass PerceptualLoss(nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n        vgg16_model = vgg16()\n        # Load the pretrained weights manually\n        vgg16_model.load_state_dict(torch.load(weights_path))\n        # Extract the features portion of VGG16\n        self.features = vgg16_model.features[:23]  # Adjust based on the layers you need\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, inpainted_image, target_image):\n        perception_loss = nn.MSELoss()\n        return perception_loss(self.vgg19(inpainted_image), self.vgg19(target_image))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T06:45:05.319316Z","iopub.execute_input":"2024-02-19T06:45:05.319599Z","iopub.status.idle":"2024-02-19T06:45:05.326271Z","shell.execute_reply.started":"2024-02-19T06:45:05.319577Z","shell.execute_reply":"2024-02-19T06:45:05.325298Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            *self.discriminator_block(3, 64, normalization=False),\n            *self.discriminator_block(64, 128),\n            *self.discriminator_block(128, 256),\n            *self.discriminator_block(256, 512),\n            nn.AdaptiveAvgPool2d(1),  # Add adaptive pooling layer to reduce to 1x1 spatial size\n            nn.Conv2d(512, 1, 1)  # Adjust to use a 1x1 kernel\n        )\n\n    def discriminator_block(self, in_filters, out_filters, normalization=True):\n        layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n        if normalization:\n            layers.append(nn.BatchNorm2d(out_filters))\n        layers.append(nn.LeakyReLU(0.2, inplace=True))\n        return layers\n\n    def forward(self, img):\n        output = self.model(img)\n        return output.view(-1, 1) ","metadata":{"execution":{"iopub.status.busy":"2024-02-19T06:45:05.327384Z","iopub.execute_input":"2024-02-19T06:45:05.327660Z","iopub.status.idle":"2024-02-19T06:45:05.340558Z","shell.execute_reply.started":"2024-02-19T06:45:05.327638Z","shell.execute_reply":"2024-02-19T06:45:05.339779Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\nclass UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, 4, 2, 1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, skip_channels, dilation, dropout=0.0):\n        super(UNetUp, self).__init__()\n        # Adjust the input channels for the transposed convolution\n        self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1)\n        self.conv = nn.Sequential(\n            nn.BatchNorm2d(out_size + skip_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_size + skip_channels, out_size, kernel_size=3, stride=1, padding=dilation, dilation=dilation),\n            nn.BatchNorm2d(out_size),\n            nn.ReLU(inplace=True)\n        )\n        if dropout:\n            self.conv.add_module(\"dropout\", nn.Dropout(dropout))\n\n    def forward(self, x, skip_input):\n        x = self.up(x)\n        x = torch.cat((x, skip_input), 1)\n        x = self.conv(x)\n        return x\n\n\nclass UNetGenerator(nn.Module):\n    def __init__(self):\n        super(UNetGenerator, self).__init__()\n        self.down1 = UNetDown(3, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512)\n        self.down5 = UNetDown(512, 1024)\n        self.up1 = UNetUp(in_size=1024, out_size=512, skip_channels=512, dilation=2, dropout=0.5)\n        self.up2 = UNetUp(in_size=512, out_size=256, skip_channels=256, dilation=2, dropout=0.5)\n        self.up3 = UNetUp(in_size=256, out_size=128, skip_channels=128, dilation=2, dropout=0.0)\n        self.up4 = UNetUp(in_size=128, out_size=64, skip_channels=64, dilation=2, dropout=0.0)\n        # Adjust the number of output channels in the final layer to match your input image\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(64, 3, 4, 2, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x, mask):\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        u1 = self.up1(d5, d4)\n        u2 = self.up2(u1, d3)\n        u3 = self.up3(u2, d2)\n        u4 = self.up4(u3, d1)\n        u4 = self.final(u4)\n        return u4 * mask + x * (1 - mask)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T06:45:05.341511Z","iopub.execute_input":"2024-02-19T06:45:05.341780Z","iopub.status.idle":"2024-02-19T06:45:05.359835Z","shell.execute_reply.started":"2024-02-19T06:45:05.341759Z","shell.execute_reply":"2024-02-19T06:45:05.358940Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class RefinementNetwork(nn.Module):\n    def __init__(self):\n        super(RefinementNetwork, self).__init__()\n        self.refinement_layers = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 3, kernel_size=3, stride=1, padding=1),\n            nn.Tanh()  # Ensuring the output is within the same range as the U-Net generator output\n        )\n    \n    def forward(self, x):\n        return self.refinement_layers(x)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T06:45:05.360969Z","iopub.execute_input":"2024-02-19T06:45:05.361333Z","iopub.status.idle":"2024-02-19T06:45:05.375573Z","shell.execute_reply.started":"2024-02-19T06:45:05.361302Z","shell.execute_reply":"2024-02-19T06:45:05.374731Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def masked_l1_loss(output, target, mask):\n    \"\"\"\n    Calculate L1 loss only for the masked regions.\n    \n    Parameters:\n    - output: the output from the generator (inpainted image).\n    - target: the ground truth image.\n    - mask: the binary mask indicating the regions to inpaint (1 for missing regions).\n    \n    Returns:\n    - The L1 loss computed only for the masked regions.\n    \"\"\"\n    # Ensure the mask is in the correct format (same size as output/target and binary)\n    mask = mask.expand_as(target)  # Expanding the mask to match the target dimensions if needed\n    \n    # Calculate the difference only in the masked regions\n    difference = (output - target) * mask  # Apply mask to the difference\n    \n    # Calculate the L1 loss only for the masked regions\n    loss = torch.abs(difference).sum() / mask.sum()  # Normalize by the number of masked pixels\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-02-19T06:45:05.376661Z","iopub.execute_input":"2024-02-19T06:45:05.376919Z","iopub.status.idle":"2024-02-19T06:45:05.385863Z","shell.execute_reply.started":"2024-02-19T06:45:05.376897Z","shell.execute_reply":"2024-02-19T06:45:05.384985Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Following is the implementation from 9th paper of my folder","metadata":{}},{"cell_type":"code","source":"generator = UNetGenerator().to(device)\ndiscriminator_d1 = Discriminator().to(device)\nvgg16_feature_extractor = VGG16FeatureExtractor(weights_path).to(device)  # Used within ContentStyleLoss\nperceptual_loss = PerceptualLoss().to(device)  # Optional based on your preference\ncriterion_gan= nn.BCEWithLogitsLoss()\nrefinement_network = RefinementNetwork().to(device)\n\n\n'''generator_path = '/kaggle/input/weight/generator_11_1000.pth'\ndiscriminator_path = '/kaggle/input/weight/discriminator_11_1000.pth'\ngenerator.load_state_dict(torch.load(generator_path))\ndiscriminator_d1.load_state_dict(torch.load(discriminator_path))\n'''\n# Define an optimizer for the refinement network\noptimizer_refinement = torch.optim.Adam(refinement_network.parameters(), lr=1e-4)\noptimizer_g = torch.optim.Adam(generator.parameters(), lr=3e-4)\noptimizer_d1 = torch.optim.Adam(discriminator_d1.parameters(), lr=3e-4)\n\n# Learning rate scheduler for decay\nscheduler_g = torch.optim.lr_scheduler.StepLR(optimizer_g, step_size=2, gamma=0.5)\nscheduler_d1 = torch.optim.lr_scheduler.StepLR(optimizer_d1, step_size=2, gamma=0.5)\nscheduler_rf = torch.optim.lr_scheduler.StepLR(optimizer_refinement, step_size=2, gamma=0.5)\n\nfrom PIL import Image\nimport os\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Training Loop\nnum_epochs = 100\nalpha = 0.1  # Weight for content loss\nbeta = 0.2   # Weight for style loss\nlambda_gp = 10\nsave_interval=500\ncriterion_pixelwise = nn.L1Loss()\n\nfor epoch in range(num_epochs):\n    generator.train()\n    total_train_loss = 0.0\n    for i, batch in enumerate(train_dataloader):\n        if batch is None:\n            continue\n        real_images = batch['ground_truth'].to(device)\n        masked_images = batch['weighted_masked_image'].to(device)\n        masks = batch['mask'].to(device)\n        # Generate fake images\n        fake_imgs = generator(real_images, masks)\n        refined_images = refinement_network(fake_imgs)\n        # ---------------------\n        #  Train Discriminator D1\n        # ---------------------\n        #optimizer_d1.zero_grad()\n        \n       \n        #discriminator_d1.zero_grad()\n\n        real_loss = criterion_gan(discriminator_d1(real_images), torch.ones(real_images.size(0), 1, device=real_images.device))\n        fake_loss = criterion_gan(discriminator_d1(fake_imgs.detach()), torch.zeros(fake_imgs.size(0), 1, device=fake_imgs.device))\n\n        #gradient_penalty = compute_gradient_penalty(discriminator_d1, real_images, fake_imgs)\n        #d_loss = -(torch.mean(real_loss) - torch.mean(fake_loss)) + lambda_gp * gradient_penalty\n        d_loss=(real_loss+fake_loss)/2\n        \n        optimizer_d1.zero_grad()\n        d_loss.backward(retain_graph=True)\n        optimizer_d1.step()\n\n        # Train Generator\n        \n\n        g_loss = criterion_gan(discriminator_d1(fake_imgs), torch.ones(fake_imgs.size(0), 1, device=fake_imgs.device))\n        \n        #pixel_loss = criterion_pixelwise(fake_imgs, real_images)\n        \n        \n        pixel_loss = masked_l1_loss(refined_images, real_images, masks) #L1 loss counted only on the masked region\n        content_loss, style_loss = content_style_loss(refined_images, real_images)\n        \n        total_loss = (6 * pixel_loss) + (0.1 * content_loss) + (100 * style_loss) + (0.05 * g_loss)         \n        \n        \n        optimizer_g.zero_grad()\n        optimizer_refinement.zero_grad()\n        \n        # Backpropagate total loss\n        total_loss.backward()\n        \n        # Step both optimizers\n        optimizer_g.step()\n        optimizer_refinement.step()\n        \n        if i % save_interval == 0:\n            generator_path = f'generator_{epoch}_{i}.pth'\n            discriminator_path = f'discriminator_{epoch}_{i}.pth'\n            refinement_path = f'refinement_{epoch}_{i}.pth'\n            torch.save(generator.state_dict(), generator_path)\n            torch.save(refinement_network.state_dict(), refinement_path)\n            torch.save(discriminator_d1.state_dict(), discriminator_path)\n        if i % 100 == 0 and i>0:\n            # Update learning rate\n            generator.eval()\n            refinement_network.eval()  # Ensure the refinement network is in eval mode\n\n            total_val_psnr = 0.0\n            total_val_ssim = 0.0\n            with torch.no_grad():\n                for batch in val_dataloader:\n                    real_images = batch['ground_truth'].to(device)\n                    masked_images = batch['weighted_masked_image'].to(device)\n                    masks = batch['mask'].to(device)\n\n                    # Inpaint with the generator\n                    inpainted_images = generator(masked_images, masks)\n\n                    # Refine the inpainted images\n                    refined_images = refinement_network(inpainted_images)  # Use the refinement network here\n\n                    # Normalize images if necessary\n                    real_images = (real_images + 1) / 2\n                    refined_images = (refined_images + 1) / 2  # Make sure to use refined_images here\n\n                    batch_psnr = calculate_psnr(real_images, refined_images)  # Use refined_images for comparison\n\n                    total_val_psnr += batch_psnr.item()  # Make sure to extract the item if it's a tensor\n\n            avg_val_psnr = total_val_psnr / len(val_dataloader)\n            print(f\"Epoch {epoch}: Avg. PSNR: {avg_val_psnr:.2f}\")\n            print(f\"Epoch {epoch}/{num_epochs} - D1 Loss: {d_loss.item()} - Total Loss: {total_loss.item()}\")\n            print(f\"content loss {content_loss.item()}- style loss:{style_loss.item()}- pixel loss:{pixel_loss.item()}\")\n\n    # Switch back to training mode\n    generator.train()\n    refinement_network.train()      \n    scheduler_g.step()\n    scheduler_d1.step()\n    scheduler_rf.step()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T06:45:05.388859Z","iopub.execute_input":"2024-02-19T06:45:05.389229Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 0: Avg. PSNR: 8.10\nEpoch 0/100 - D1 Loss: 0.4816051125526428 - Total Loss: 2.988987445831299\ncontent loss 2.2459826469421387- style loss:2.5414409465795984e-10- pixel loss:0.45233154296875\nEpoch 0: Avg. PSNR: 8.61\nEpoch 0/100 - D1 Loss: 0.3614780008792877 - Total Loss: 2.6368229389190674\ncontent loss 1.9853142499923706- style loss:2.1982121678476574e-10- pixel loss:0.3943445086479187\nEpoch 0: Avg. PSNR: 8.77\nEpoch 0/100 - D1 Loss: 0.5773820877075195 - Total Loss: 2.4222967624664307\ncontent loss 2.005720376968384- style loss:2.363539919336688e-10- pixel loss:0.3614867925643921\nEpoch 0: Avg. PSNR: 8.84\nEpoch 0/100 - D1 Loss: 0.5982439517974854 - Total Loss: 2.8595287799835205\ncontent loss 1.8206961154937744- style loss:2.175808422322234e-10- pixel loss:0.4393429458141327\nEpoch 0: Avg. PSNR: 9.02\nEpoch 0/100 - D1 Loss: 0.7011483907699585 - Total Loss: 2.619135618209839\ncontent loss 1.5381457805633545- style loss:1.4514360546780125e-10- pixel loss:0.39750948548316956\nEpoch 0: Avg. PSNR: 9.21\nEpoch 0/100 - D1 Loss: 0.36549896001815796 - Total Loss: 2.3666412830352783\ncontent loss 1.5542752742767334- style loss:1.718072489831357e-10- pixel loss:0.35387516021728516\nEpoch 0: Avg. PSNR: 9.26\nEpoch 0/100 - D1 Loss: 0.2795448303222656 - Total Loss: 2.5917258262634277\ncontent loss 1.7390979528427124- style loss:2.0818902157770935e-10- pixel loss:0.3894510865211487\nEpoch 0: Avg. PSNR: 9.31\nEpoch 0/100 - D1 Loss: 0.5849064588546753 - Total Loss: 2.004856586456299\ncontent loss 1.7179570198059082- style loss:1.7646176186936202e-10- pixel loss:0.3007643222808838\nEpoch 0: Avg. PSNR: 9.34\nEpoch 0/100 - D1 Loss: 0.3909810185432434 - Total Loss: 2.4834372997283936\ncontent loss 1.8335002660751343- style loss:2.1992443977048026e-10- pixel loss:0.37318310141563416\nEpoch 0: Avg. PSNR: 9.45\nEpoch 0/100 - D1 Loss: 0.45222240686416626 - Total Loss: 2.441976547241211\ncontent loss 1.6458526849746704- style loss:1.8403377433084955e-10- pixel loss:0.36981648206710815\nEpoch 0: Avg. PSNR: 9.51\nEpoch 0/100 - D1 Loss: 0.3994349539279938 - Total Loss: 2.32798171043396\ncontent loss 1.6627930402755737- style loss:1.7734502755217818e-10- pixel loss:0.34972497820854187\nEpoch 0: Avg. PSNR: 9.52\nEpoch 0/100 - D1 Loss: 0.395672470331192 - Total Loss: 2.0739617347717285\ncontent loss 1.7340518236160278- style loss:1.8671450496832165e-10- pixel loss:0.3079436123371124\nEpoch 0: Avg. PSNR: 9.63\nEpoch 0/100 - D1 Loss: 0.393002450466156 - Total Loss: 2.3553426265716553\ncontent loss 1.8161529302597046- style loss:2.2031251822873799e-10- pixel loss:0.3532956540584564\nEpoch 0: Avg. PSNR: 9.69\nEpoch 0/100 - D1 Loss: 0.755556583404541 - Total Loss: 2.036760091781616\ncontent loss 1.6403847932815552- style loss:1.900799517784435e-10- pixel loss:0.30582356452941895\nEpoch 1: Avg. PSNR: 9.58\nEpoch 1/100 - D1 Loss: 0.4677318334579468 - Total Loss: 2.332061529159546\ncontent loss 1.6058614253997803- style loss:1.7687405706734438e-10- pixel loss:0.35013115406036377\nEpoch 1: Avg. PSNR: 9.57\nEpoch 1/100 - D1 Loss: 0.5169967412948608 - Total Loss: 2.195729970932007\ncontent loss 1.5357304811477661- style loss:1.611121652755898e-10- pixel loss:0.3344345986843109\nEpoch 1: Avg. PSNR: 9.62\nEpoch 1/100 - D1 Loss: 0.5987540483474731 - Total Loss: 2.4079995155334473\ncontent loss 1.7468925714492798- style loss:1.8705600957069635e-10- pixel loss:0.36011481285095215\nEpoch 1: Avg. PSNR: 9.68\nEpoch 1/100 - D1 Loss: 0.5730600357055664 - Total Loss: 2.4927573204040527\ncontent loss 1.890868067741394- style loss:2.31237584635835e-10- pixel loss:0.37385958433151245\nEpoch 1: Avg. PSNR: 9.66\nEpoch 1/100 - D1 Loss: 0.7205192446708679 - Total Loss: 2.016902208328247\ncontent loss 1.647953987121582- style loss:1.8909922239185306e-10- pixel loss:0.3029761016368866\nEpoch 1: Avg. PSNR: 9.70\nEpoch 1/100 - D1 Loss: 0.7132905721664429 - Total Loss: 2.5815067291259766\ncontent loss 1.8391931056976318- style loss:2.781907482152235e-10- pixel loss:0.3843958079814911\n","output_type":"stream"}]}]}