{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7625342,"sourceType":"datasetVersion","datasetId":4442249},{"sourceId":7625566,"sourceType":"datasetVersion","datasetId":4442422},{"sourceId":7639623,"sourceType":"datasetVersion","datasetId":4452417},{"sourceId":7641569,"sourceType":"datasetVersion","datasetId":4453704}],"dockerImageVersionId":30648,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport tensorflow as tf\nfrom PIL import Image, ImageFile\nimport os\nimport random\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision.transforms.functional as TF\nimport random\nimport cv2\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-17T16:08:07.267733Z","iopub.execute_input":"2024-02-17T16:08:07.268095Z","iopub.status.idle":"2024-02-17T16:08:07.276904Z","shell.execute_reply.started":"2024-02-17T16:08:07.268067Z","shell.execute_reply":"2024-02-17T16:08:07.275981Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"!pip install torch\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:08:11.034168Z","iopub.execute_input":"2024-02-17T16:08:11.034532Z","iopub.status.idle":"2024-02-17T16:08:42.947087Z","shell.execute_reply.started":"2024-02-17T16:08:11.034503Z","shell.execute_reply":"2024-02-17T16:08:42.946078Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.12.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim\n\ndef calculate_psnr(target, prediction, max_pixel=1.0):\n    mse = torch.mean((target - prediction) ** 2)\n    if mse == 0:\n        return float('inf')\n    return 20 * torch.log10(max_pixel / torch.sqrt(mse))\n\ndef calculate_ssim(target, prediction, data_range=1.0, channel_axis=-1):\n    # Convert tensors to numpy arrays\n    target_np = target.cpu().detach().numpy()\n    prediction_np = prediction.cpu().detach().numpy()\n    # Calculate SSIM over the batch\n    ssim_val = np.mean([ssim(t, p, data_range=data_range, channel_axis=channel_axis) for t, p in zip(target_np, prediction_np)])\n    return ssim_val","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:09:10.348374Z","iopub.execute_input":"2024-02-17T16:09:10.348760Z","iopub.status.idle":"2024-02-17T16:09:10.357348Z","shell.execute_reply.started":"2024-02-17T16:09:10.348721Z","shell.execute_reply":"2024-02-17T16:09:10.356391Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif device:\n    print('Model is running on GPU:', device)\nelse:\n    print('Model is running on CPU')\n\nclass InpaintingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.image_list = os.listdir(image_dir)\n        self.mask_list = os.listdir(mask_dir)\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n\n        # Ensure the lists are sorted so they correspond\n        self.image_list.sort()\n        self.mask_list.sort()\n\n    def __len__(self):\n        return len(self.image_list)\n    \n    def dilate_mask(self, mask, dilation_kernel_size=3):\n        kernel = np.ones((dilation_kernel_size, dilation_kernel_size), np.uint8)\n        dilated_mask = cv2.dilate(mask.numpy(), kernel, iterations=1)\n        return torch.from_numpy(dilated_mask)\n\n    def create_weight_map(self, mask, dilated_mask, border_weight=2.0):\n        border = dilated_mask - mask\n        weight_map = torch.ones_like(mask)\n        weight_map[border == 1] = border_weight\n        return weight_map  # Add this line\n\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(self.image_dir, self.image_list[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n\n        image = Image.open(image_path).convert('RGB')\n        mask = Image.open(mask_path).convert('1')\n\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n\n        # Ensure mask is a binary tensor with the same size as image in the channel dimension\n        mask = mask.expand_as(image)\n\n        masked_image = image * (1 - mask)\n        \n        mask = (mask > 0).float()\n\n        masked_image = image * (1 - mask)\n\n        dilated_mask = self.dilate_mask(mask)\n        weight_map = self.create_weight_map(mask, dilated_mask)\n        \n        \n        \n        weighted_masked_image = masked_image * weight_map\n\n        return {\n            'ground_truth': image, \n            'weighted_masked_image': weighted_masked_image, \n            'mask': dilated_mask\n            }\n\n\n\nimage_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    \n])\n\n# Define your mask transformations including random rotation, flip, and dilation\nmask_transform = transforms.Compose([\n    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n])\n\n# Create dataset instances\ntrain_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/trainingimage/new_subdataset',\n    mask_dir='/kaggle/input/mask-dataset/training',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n)\n\nval_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/valimages/ValPlaces2',\n    mask_dir='/kaggle/input/validation-mask',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n)\n\nfrom PIL import Image\nimport os\nimport math\nfrom torch.utils.data import DataLoader, random_split\n\ntotal_size = len(train_dataset)\n\n\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:09:13.782959Z","iopub.execute_input":"2024-02-17T16:09:13.783600Z","iopub.status.idle":"2024-02-17T16:09:13.892686Z","shell.execute_reply.started":"2024-02-17T16:09:13.783571Z","shell.execute_reply":"2024-02-17T16:09:13.891730Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stdout","text":"Model is running on GPU: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.models import vgg16, inception_v3, VGG16_Weights, Inception_V3_Weights\nclass VGG16FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(VGG16FeatureExtractor, self).__init__()\n        vgg16 = torchvision.models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\n        self.features = nn.Sequential(*list(vgg16.children())[:23])  # Adjust based on the layers you need\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        return self.features(x)\n\nclass ContentStyleLoss(nn.Module):\n    def __init__(self):\n        super(ContentStyleLoss, self).__init__()\n        self.feature_extractor = VGG16FeatureExtractor()\n\n    def compute_gram_matrix(self, input):\n        a, b, c, d = input.size()  # a=batch size(=1)\n        features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n        G = torch.mm(features, features.t())  # compute the gram product\n        return G.div(a * b * c * d)\n\n    def forward(self, generated, target):\n        gen_features = self.feature_extractor(generated)\n        target_features = self.feature_extractor(target)\n        content_loss = F.mse_loss(gen_features, target_features)\n\n        # Compute style loss\n        gen_gram = self.compute_gram_matrix(gen_features)\n        target_gram = self.compute_gram_matrix(target_features)\n        style_loss = F.mse_loss(gen_gram, target_gram)\n\n        return content_loss, style_loss","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:09:18.559654Z","iopub.execute_input":"2024-02-17T16:09:18.560478Z","iopub.status.idle":"2024-02-17T16:09:18.570523Z","shell.execute_reply.started":"2024-02-17T16:09:18.560447Z","shell.execute_reply":"2024-02-17T16:09:18.569686Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"\nclass PerceptualLoss(nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n        vgg19 = torchvision.models.vgg19(pretrained=True).features\n        self.vgg19 = nn.Sequential(*list(vgg19.children())[:36]).eval()  # Up to the second conv layer in the 5th block\n        for param in self.vgg19.parameters():\n            param.requires_grad = False\n\n    def forward(self, inpainted_image, target_image):\n        perception_loss = nn.MSELoss()\n        return perception_loss(self.vgg19(inpainted_image), self.vgg19(target_image))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:09:23.115716Z","iopub.execute_input":"2024-02-17T16:09:23.116585Z","iopub.status.idle":"2024-02-17T16:09:23.122911Z","shell.execute_reply.started":"2024-02-17T16:09:23.116553Z","shell.execute_reply":"2024-02-17T16:09:23.121929Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"\n'''class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            # Simple discriminator model\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),  # Adjust the kernel size/padding for final size\n            nn.Flatten(),  # Flatten the output to [batch_size, 1]\n            nn.Sigmoid()\n        )\n        \n    def forward(self, img):\n        validity = self.model(img)\n        return validity'''","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:05:34.984074Z","iopub.execute_input":"2024-02-17T16:05:34.984412Z","iopub.status.idle":"2024-02-17T16:05:34.993290Z","shell.execute_reply.started":"2024-02-17T16:05:34.984389Z","shell.execute_reply":"2024-02-17T16:05:34.992522Z"},"trusted":true},"execution_count":52,"outputs":[{"execution_count":52,"output_type":"execute_result","data":{"text/plain":"'class Discriminator(nn.Module):\\n    def __init__(self):\\n        super(Discriminator, self).__init__()\\n        self.model = nn.Sequential(\\n            # Simple discriminator model\\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\\n            nn.BatchNorm2d(128),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\\n            nn.BatchNorm2d(256),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\\n            nn.BatchNorm2d(512),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),  # Adjust the kernel size/padding for final size\\n            nn.Flatten(),  # Flatten the output to [batch_size, 1]\\n            nn.Sigmoid()\\n        )\\n        \\n    def forward(self, img):\\n        validity = self.model(img)\\n        return validity'"},"metadata":{}}]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            # Initial convolution layer\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Subsequent convolutional layers\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Adaptive pooling layer added to ensure the feature map is reduced to 1x1\n            nn.AdaptiveAvgPool2d(1),\n            \n            # Final convolutional layer to produce a single scalar output\n            nn.Conv2d(512, 1, kernel_size=1),\n            nn.Flatten(),  # Flatten the output to ensure it is a scalar\n            nn.Sigmoid()  # Sigmoid activation to obtain a probability\n        )\n        \n    def forward(self, img):\n        validity = self.model(img)\n        return validity\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:09:32.196193Z","iopub.execute_input":"2024-02-17T16:09:32.196562Z","iopub.status.idle":"2024-02-17T16:09:32.206178Z","shell.execute_reply.started":"2024-02-17T16:09:32.196533Z","shell.execute_reply":"2024-02-17T16:09:32.205297Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"\n'''\nclass UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_size),\n            nn.ReLU(inplace=True)\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.down1 = UNetDown(3, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x, mask):\n        # Encoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n\n        # Decoder\n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        inpainted = self.final(u7)\n        \n        # Blend the inpainted output with the original image outside the masked region\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\n        output = (1 - mask) * x + mask * inpainted\n        return self.final(u7)'''","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:05:35.005287Z","iopub.execute_input":"2024-02-17T16:05:35.005550Z","iopub.status.idle":"2024-02-17T16:05:35.019611Z","shell.execute_reply.started":"2024-02-17T16:05:35.005528Z","shell.execute_reply":"2024-02-17T16:05:35.018898Z"},"trusted":true},"execution_count":54,"outputs":[{"execution_count":54,"output_type":"execute_result","data":{"text/plain":"'\\nclass UNetDown(nn.Module):\\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\\n        super(UNetDown, self).__init__()\\n        layers = [nn.Conv2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False)]\\n        if normalize:\\n            layers.append(nn.BatchNorm2d(out_size))\\n        layers.append(nn.LeakyReLU(0.2))\\n        if dropout:\\n            layers.append(nn.Dropout(dropout))\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x):\\n        return self.model(x)\\n\\nclass UNetUp(nn.Module):\\n    def __init__(self, in_size, out_size, dropout=0.0):\\n        super(UNetUp, self).__init__()\\n        layers = [\\n            nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False),\\n            nn.BatchNorm2d(out_size),\\n            nn.ReLU(inplace=True)\\n        ]\\n        if dropout:\\n            layers.append(nn.Dropout(dropout))\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x, skip_input):\\n        x = self.model(x)\\n        x = torch.cat((x, skip_input), 1)\\n        return x\\n\\nclass UNet(nn.Module):\\n    def __init__(self):\\n        super(UNet, self).__init__()\\n\\n        self.down1 = UNetDown(3, 64, normalize=False)\\n        self.down2 = UNetDown(64, 128)\\n        self.down3 = UNetDown(128, 256)\\n        self.down4 = UNetDown(256, 512, dropout=0.5)\\n        self.down5 = UNetDown(512, 512, dropout=0.5)\\n        self.down6 = UNetDown(512, 512, dropout=0.5)\\n        self.down7 = UNetDown(512, 512, dropout=0.5)\\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\\n\\n        self.up1 = UNetUp(512, 512, dropout=0.5)\\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\\n        self.up5 = UNetUp(1024, 256)\\n        self.up6 = UNetUp(512, 128)\\n        self.up7 = UNetUp(256, 64)\\n\\n        self.final = nn.Sequential(\\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\\n            nn.Tanh()\\n        )\\n\\n    def forward(self, x, mask):\\n        # Encoder\\n        d1 = self.down1(x)\\n        d2 = self.down2(d1)\\n        d3 = self.down3(d2)\\n        d4 = self.down4(d3)\\n        d5 = self.down5(d4)\\n        d6 = self.down6(d5)\\n        d7 = self.down7(d6)\\n        d8 = self.down8(d7)\\n\\n        # Decoder\\n        u1 = self.up1(d8, d7)\\n        u2 = self.up2(u1, d6)\\n        u3 = self.up3(u2, d5)\\n        u4 = self.up4(u3, d4)\\n        u5 = self.up5(u4, d3)\\n        u6 = self.up6(u5, d2)\\n        u7 = self.up7(u6, d1)\\n\\n        inpainted = self.final(u7)\\n        \\n        # Blend the inpainted output with the original image outside the masked region\\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\\n        output = (1 - mask) * x + mask * inpainted\\n        return self.final(u7)'"},"metadata":{}}]},{"cell_type":"code","source":"'''class PartialConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n        super(PartialConv2d, self).__init__()\n        self.input_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n        self.mask_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, False)\n        self.mask_conv.weight.data.fill_(1.0)\n        for param in self.mask_conv.parameters():\n            param.requires_grad = False\n\n    def forward(self, x, mask):\n        output = self.input_conv(x * mask)\n        if self.input_conv.bias is not None:\n            output_bias = self.input_conv.bias.view(1, -1, 1, 1).expand_as(output)\n        else:\n            output_bias = torch.zeros_like(output)\n        \n        with torch.no_grad():\n            output_mask = self.mask_conv(mask)\n\n        mask_ratio = self.mask_conv.weight.data.sum() / output_mask.sum()\n        output = mask_ratio * output - output_bias * mask_ratio + output_bias\n        new_mask = torch.ones_like(output_mask)\n        new_mask[output_mask == 0] = 0\n\n        return output, new_mask\n'''","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:05:35.020784Z","iopub.execute_input":"2024-02-17T16:05:35.021072Z","iopub.status.idle":"2024-02-17T16:05:35.033731Z","shell.execute_reply.started":"2024-02-17T16:05:35.021050Z","shell.execute_reply":"2024-02-17T16:05:35.033019Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"'class PartialConv2d(nn.Module):\\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\\n        super(PartialConv2d, self).__init__()\\n        self.input_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\\n        self.mask_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, False)\\n        self.mask_conv.weight.data.fill_(1.0)\\n        for param in self.mask_conv.parameters():\\n            param.requires_grad = False\\n\\n    def forward(self, x, mask):\\n        output = self.input_conv(x * mask)\\n        if self.input_conv.bias is not None:\\n            output_bias = self.input_conv.bias.view(1, -1, 1, 1).expand_as(output)\\n        else:\\n            output_bias = torch.zeros_like(output)\\n        \\n        with torch.no_grad():\\n            output_mask = self.mask_conv(mask)\\n\\n        mask_ratio = self.mask_conv.weight.data.sum() / output_mask.sum()\\n        output = mask_ratio * output - output_bias * mask_ratio + output_bias\\n        new_mask = torch.ones_like(output_mask)\\n        new_mask[output_mask == 0] = 0\\n\\n        return output, new_mask\\n'"},"metadata":{}}]},{"cell_type":"code","source":"'''class UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        self.pc = PartialConv2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False)\n        layers = []\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, mask):\n        x, mask = self.pc(x, mask)\n        for layer in self.model:\n            x = layer(x)\n        return x, mask\nclass UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        self.up = nn.ConvTranspose2d(in_size * 2, out_size, kernel_size=4, stride=2, padding=1, bias=False)\n        self.pc = PartialConv2d(out_size, out_size, kernel_size=3, stride=1, padding=1, bias=False)\n        layers = [\n            nn.BatchNorm2d(out_size),\n            nn.ReLU(inplace=True)\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input, mask, skip_mask):\n        x = self.up(x)\n        # Correctly concatenate upsampled x with skip_input BEFORE passing through PartialConv2d\n        x = torch.cat((x, skip_input), 1)  # Ensure this concatenation doubles the channels as expected\n        mask = F.interpolate(mask, scale_factor=2, mode='nearest')\n        x, mask = self.pc(x, mask) \n        for layer in self.model:\n            x = layer(x)\n        return x, mask\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.down1 = UNetDown(3, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x, mask):\n        # Encoder\n        d1, mask1 = self.down1(x,mask)\n        d2, mask2 = self.down2(d1, mask1)\n        d3, mask3 = self.down3(d2, mask2)\n        d4, mask4 = self.down4(d3, mask3)\n        d5, mask5 = self.down5(d4, mask4)\n        d6, mask6 = self.down6(d5, mask5)\n        d7, mask7 = self.down7(d6, mask6)\n        d8, mask8 = self.down8(d7, mask7)\n\n        # Decoder\n        # Decoder\n        u1, mask_up1 = self.up1(d8, d7, mask8, mask7)  # Corrected to use d8 and mask8 as the initial inputs\n        u2, mask_up2 = self.up2(u1, d7, mask_up1, mask7)  # Proceed with correct skip connections\n        u3, mask_up3 = self.up3(u2, d6, mask_up2, mask6)\n        u4, mask_up4 = self.up4(u3, d5, mask_up3, mask5)\n        u5, mask_up5 = self.up5(u4, d4, mask_up4, mask4)\n        u6, mask_up6 = self.up6(u5, d3, mask_up5, mask3)\n        u7, mask_up7 = self.up7(u6, d2, mask_up6, mask2)\n        final_output, final_mask = self.final(u7, mask_up7)\n\n        # Blend the inpainted output with the original image outside the masked region\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\n        output = (1 - mask) * x + mask * inpainted\n        return self.final(u7)'''","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:05:35.034887Z","iopub.execute_input":"2024-02-17T16:05:35.035153Z","iopub.status.idle":"2024-02-17T16:05:35.045974Z","shell.execute_reply.started":"2024-02-17T16:05:35.035131Z","shell.execute_reply":"2024-02-17T16:05:35.045113Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"\"class UNetDown(nn.Module):\\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\\n        super(UNetDown, self).__init__()\\n        self.pc = PartialConv2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False)\\n        layers = []\\n        if normalize:\\n            layers.append(nn.BatchNorm2d(out_size))\\n        layers.append(nn.LeakyReLU(0.2))\\n        if dropout:\\n            layers.append(nn.Dropout(dropout))\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x, mask):\\n        x, mask = self.pc(x, mask)\\n        for layer in self.model:\\n            x = layer(x)\\n        return x, mask\\nclass UNetUp(nn.Module):\\n    def __init__(self, in_size, out_size, dropout=0.0):\\n        super(UNetUp, self).__init__()\\n        self.up = nn.ConvTranspose2d(in_size * 2, out_size, kernel_size=4, stride=2, padding=1, bias=False)\\n        self.pc = PartialConv2d(out_size, out_size, kernel_size=3, stride=1, padding=1, bias=False)\\n        layers = [\\n            nn.BatchNorm2d(out_size),\\n            nn.ReLU(inplace=True)\\n        ]\\n        if dropout:\\n            layers.append(nn.Dropout(dropout))\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x, skip_input, mask, skip_mask):\\n        x = self.up(x)\\n        # Correctly concatenate upsampled x with skip_input BEFORE passing through PartialConv2d\\n        x = torch.cat((x, skip_input), 1)  # Ensure this concatenation doubles the channels as expected\\n        mask = F.interpolate(mask, scale_factor=2, mode='nearest')\\n        x, mask = self.pc(x, mask) \\n        for layer in self.model:\\n            x = layer(x)\\n        return x, mask\\nclass UNet(nn.Module):\\n    def __init__(self):\\n        super(UNet, self).__init__()\\n\\n        self.down1 = UNetDown(3, 64, normalize=False)\\n        self.down2 = UNetDown(64, 128)\\n        self.down3 = UNetDown(128, 256)\\n        self.down4 = UNetDown(256, 512, dropout=0.5)\\n        self.down5 = UNetDown(512, 512, dropout=0.5)\\n        self.down6 = UNetDown(512, 512, dropout=0.5)\\n        self.down7 = UNetDown(512, 512, dropout=0.5)\\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\\n\\n        self.up1 = UNetUp(512, 512, dropout=0.5)\\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\\n        self.up5 = UNetUp(1024, 256)\\n        self.up6 = UNetUp(512, 128)\\n        self.up7 = UNetUp(256, 64)\\n\\n        self.final = nn.Sequential(\\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\\n            nn.Tanh()\\n        )\\n\\n    def forward(self, x, mask):\\n        # Encoder\\n        d1, mask1 = self.down1(x,mask)\\n        d2, mask2 = self.down2(d1, mask1)\\n        d3, mask3 = self.down3(d2, mask2)\\n        d4, mask4 = self.down4(d3, mask3)\\n        d5, mask5 = self.down5(d4, mask4)\\n        d6, mask6 = self.down6(d5, mask5)\\n        d7, mask7 = self.down7(d6, mask6)\\n        d8, mask8 = self.down8(d7, mask7)\\n\\n        # Decoder\\n        # Decoder\\n        u1, mask_up1 = self.up1(d8, d7, mask8, mask7)  # Corrected to use d8 and mask8 as the initial inputs\\n        u2, mask_up2 = self.up2(u1, d7, mask_up1, mask7)  # Proceed with correct skip connections\\n        u3, mask_up3 = self.up3(u2, d6, mask_up2, mask6)\\n        u4, mask_up4 = self.up4(u3, d5, mask_up3, mask5)\\n        u5, mask_up5 = self.up5(u4, d4, mask_up4, mask4)\\n        u6, mask_up6 = self.up6(u5, d3, mask_up5, mask3)\\n        u7, mask_up7 = self.up7(u6, d2, mask_up6, mask2)\\n        final_output, final_mask = self.final(u7, mask_up7)\\n\\n        # Blend the inpainted output with the original image outside the masked region\\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\\n        output = (1 - mask) * x + mask * inpainted\\n        return self.final(u7)\""},"metadata":{}}]},{"cell_type":"code","source":"\n\nclass PConv2D(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, bias=False):\n        super(PConv2D, self).__init__()\n        # Use a standard convolution layer\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=bias)\n        # Corrected to use Conv2d instead of a non-existent pConv2d\n        self.mask_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n        # Initialize mask conv weights to 1\n        self.mask_conv.weight.data.fill_(1.0)\n        # Prevent gradient computation for mask_conv weights\n        for param in self.mask_conv.parameters():\n            param.requires_grad = False\n\n    def forward(self, input, mask):\n        with torch.no_grad():\n            updated_mask = self.mask_conv(mask)\n        # Multiply input with mask before passing through the convolution\n        output = self.conv(input * mask)\n        # Adjust output using the updated mask to prevent division by zero\n        output = output * torch.reciprocal(updated_mask + 1e-8)\n        return output, updated_mask\nclass UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        self.model = nn.Sequential(\n            PConv2D(in_size, out_size, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_size) if normalize else nn.Identity(),\n            nn.LeakyReLU(0.2),\n            nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n        )\n\n    def forward(self, x, mask):\n        x, updated_mask = self.model[0](x, mask)\n        for layer in self.model[1:]:\n            x = layer(x)\n        return x, updated_mask\nfrom torch.nn.functional import interpolate\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_channels, out_channels, dropout=0.0):\n        super(UNetUp, self).__init__()\n        self.up = PConv2D(in_channels, out_channels, kernel_size=2, stride=2)\n        self.conv = nn.Sequential(\n            PConv2D(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Dropout(dropout),\n            PConv2D(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x, skip_input, mask=None, skip_mask=None):\n        x, updated_mask = self.up(x, mask)\n        if x.size()[2:] != skip_input.size()[2:]:\n            x = F.interpolate(x, size=skip_input.size()[2:], mode='bilinear', align_corners=False)\n            updated_mask = F.interpolate(updated_mask, size=skip_input.size()[2:], mode='bilinear', align_corners=False)\n        x = torch.cat((x, skip_input), 1)\n        updated_mask = torch.cat((updated_mask, skip_mask), 1)\n        x, updated_mask = self.conv(x, updated_mask)\n        return x, updated_mask\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.down1 = UNetDown(3, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(1024, 512, dropout=0.5)  # Adjusted for concatenated channel sizes\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n        \n        \n\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x, mask):\n        # Encoder\n        d1, mask1 = self.down1(x,mask)\n        d2, mask2 = self.down2(d1, mask1)\n        d3, mask3 = self.down3(d2, mask2)\n        d4, mask4 = self.down4(d3, mask3)\n        d5, mask5 = self.down5(d4, mask4)\n        d6, mask6 = self.down6(d5, mask5)\n        d7, mask7 = self.down7(d6, mask6)\n        d8, mask8 = self.down8(d7, mask7)\n\n        # Decoder\n        # Decoder\n        u1, mask_up1 = self.up1(d8, d7, mask8, mask7)  # Corrected to use d8 and mask8 as the initial inputs\n      # Corrected to use d8 and mask8 as the initial inputs\n        u2, mask_up2 = self.up2(u1, d7, mask_up1, mask7)  # Proceed with correct skip connections\n        u3, mask_up3 = self.up3(u2, d6, mask_up2, mask6)\n        u4, mask_up4 = self.up4(u3, d5, mask_up3, mask5)\n        u5, mask_up5 = self.up5(u4, d4, mask_up4, mask4)\n        u6, mask_up6 = self.up6(u5, d3, mask_up5, mask3)\n        u7, mask_up7 = self.up7(u6, d2, mask_up6, mask2)\n        final_output, final_mask = self.final(u7, mask_up7)\n\n        # Blend the inpainted output with the original image outside the masked region\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\n        output = (1 - mask) * x + mask * inpainted\n        return self.final(output)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:09:46.669204Z","iopub.execute_input":"2024-02-17T16:09:46.669606Z","iopub.status.idle":"2024-02-17T16:09:46.703572Z","shell.execute_reply.started":"2024-02-17T16:09:46.669576Z","shell.execute_reply":"2024-02-17T16:09:46.702510Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"\ndef compute_gradient_penalty(D, real_samples, fake_samples):\n    \"\"\"Calculates the gradient penalty for a batch of real and fake samples.\"\"\"\n    alpha = torch.rand((real_samples.size(0), 1, 1, 1), device=real_samples.device)\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates = D(interpolates)\n    fake = torch.ones(d_interpolates.size(), device=real_samples.device, requires_grad=False)\n    \n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    \n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:09:51.640820Z","iopub.execute_input":"2024-02-17T16:09:51.641521Z","iopub.status.idle":"2024-02-17T16:09:51.648576Z","shell.execute_reply.started":"2024-02-17T16:09:51.641489Z","shell.execute_reply":"2024-02-17T16:09:51.647661Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"def masked_l1_loss(output, target, mask):\n    \"\"\"\n    Calculate L1 loss only for the masked regions.\n    \n    Parameters:\n    - output: the output from the generator (inpainted image).\n    - target: the ground truth image.\n    - mask: the binary mask indicating the regions to inpaint (1 for missing regions).\n    \n    Returns:\n    - The L1 loss computed only for the masked regions.\n    \"\"\"\n    # Ensure the mask is in the correct format (same size as output/target and binary)\n    mask = mask.expand_as(target)  # Expanding the mask to match the target dimensions if needed\n    \n    # Calculate the difference only in the masked regions\n    difference = (output - target) * mask  # Apply mask to the difference\n    \n    # Calculate the L1 loss only for the masked regions\n    loss = torch.abs(difference).sum() / mask.sum()  # Normalize by the number of masked pixels\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:09:55.419370Z","iopub.execute_input":"2024-02-17T16:09:55.419734Z","iopub.status.idle":"2024-02-17T16:09:55.425402Z","shell.execute_reply.started":"2024-02-17T16:09:55.419705Z","shell.execute_reply":"2024-02-17T16:09:55.424452Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"Following is the implementation from 9th paper of my folder","metadata":{}},{"cell_type":"code","source":"'''from torchvision.models import vgg16, inception_v3, VGG16_Weights, Inception_V3_Weights\n\nvgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\nfor param in vgg.parameters():\n    param.requires_grad = False\nvgg= vgg.to(device)\n\n'''\n'''from torchvision.models import vgg16\n\nvgg = vgg16(weights=VGG16_Weights.IMAGENET1K_V1).features\nfor param in vgg.parameters():\n    param.requires_grad = False\n    \ndef gram_matrix(input):\n    if input.dim() == 3:\n        input = input.unsqueeze(0)  # Add a batch dimension if missing\n    a, b, c, d = input.size()\n    features = input.view(a * b, c * d)\n    G = torch.mm(features, features.t())\n    return G.div(a * b * c * d)\n\ndef style_loss(output, target):\n    loss = 0\n    for out_feat, tgt_feat in zip(vgg(output), vgg(target)):\n        out_gram = gram_matrix(out_feat)\n        tgt_gram = gram_matrix(tgt_feat)\n        loss += F.l1_loss(out_gram, tgt_gram)\n    return loss'''\n\n'''def compute_gram_matrix(input):\n    a, b, c, d = input.size()  # a=batch size(=1)\n    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n    G = torch.mm(features, features.t())  # compute the gram product\n    return G.div(a * b * c * d)\n\ndef style_loss(generated, target):\n    gen_features = self.feature_extractor(generated)\n    target_features = self.feature_extractor(target)\n    content_loss = F.mse_loss(gen_features, target_features)\n\n    # Compute style loss\n    gen_gram = self.compute_gram_matrix(gen_features)\n    target_gram = self.compute_gram_matrix(target_features)\n    style_loss = F.mse_loss(gen_gram, target_gram)\n    \n    return style_loss'''\n\ndef total_variation_loss(img, weight=0.1):\n    \"\"\"\n    Compute the Total Variation Loss.\n    \n    Parameters:\n    - img: Tensor, the input image of shape (N, C, H, W) where\n      N is the batch size,\n      C is the number of channels,\n      H is the height, and\n      W is the width.\n    - weight: float, the weight of the total variation loss.\n    \n    Returns:\n    - loss: Tensor, the total variation loss.\n    \"\"\"\n    # Calculate the differences between adjacent pixels\n    pixel_diff_y = torch.abs(img[:, :, 1:] - img[:, :, :-1])\n    pixel_diff_x = torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1])\n\n    # Sum up the differences\n    sum_pixel_diff = torch.sum(pixel_diff_y) + torch.sum(pixel_diff_x)\n\n    # Average over the batch and apply weight\n    loss = weight * sum_pixel_diff / img.shape[0]\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:09:59.286501Z","iopub.execute_input":"2024-02-17T16:09:59.287242Z","iopub.status.idle":"2024-02-17T16:09:59.295503Z","shell.execute_reply.started":"2024-02-17T16:09:59.287213Z","shell.execute_reply":"2024-02-17T16:09:59.294550Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"generator = UNet().to(device)\ndiscriminator_d1 = Discriminator().to(device)\nvgg16_feature_extractor = VGG16FeatureExtractor().to(device)  # Used within ContentStyleLoss\ncontent_style_loss = ContentStyleLoss().to(device)\nperceptual_loss = PerceptualLoss().to(device)  # Optional based on your preference\ncriterion_gan= nn.BCEWithLogitsLoss()\n\n# Optimizers\noptimizer_g = torch.optim.Adam(generator.parameters(), lr=3e-4)\noptimizer_d1 = torch.optim.Adam(discriminator_d1.parameters(), lr=3e-4)\n\n# Learning rate scheduler for decay\nscheduler_g = torch.optim.lr_scheduler.StepLR(optimizer_g, step_size=2, gamma=0.5)\nscheduler_d1 = torch.optim.lr_scheduler.StepLR(optimizer_d1, step_size=2, gamma=0.5)\n\nfrom PIL import Image\nimport os\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Training Loop\nnum_epochs = 100\nalpha = 0.1  # Weight for content loss\nbeta = 0.2   # Weight for style loss\nlambda_gp = 10\nsave_interval=100\ncriterion_pixelwise = nn.L1Loss()\nfor epoch in range(num_epochs):\n    generator.train()\n    total_train_loss = 0.0\n    for i, batch in enumerate(train_dataloader):\n        if batch is None:\n            continue\n        real_images = batch['ground_truth'].to(device)\n        masked_images = batch['weighted_masked_image'].to(device)\n        masks = batch['mask'].to(device)\n        # Generate fake images\n        fake_imgs = generator(real_images, masks)\n\n        # ---------------------\n        #  Train Discriminator D1\n        # ---------------------\n        #optimizer_d1.zero_grad()\n        \n       \n        #discriminator_d1.zero_grad()\n\n        real_loss = criterion_gan(discriminator_d1(real_images), torch.ones(real_images.size(0), 1, device=real_images.device))\n        fake_loss = criterion_gan(discriminator_d1(fake_imgs.detach()), torch.zeros(fake_imgs.size(0), 1, device=fake_imgs.device))\n\n        #gradient_penalty = compute_gradient_penalty(discriminator_d1, real_images, fake_imgs)\n        #d_loss = -(torch.mean(real_loss) - torch.mean(fake_loss)) + lambda_gp * gradient_penalty\n        d_loss=(real_loss+fake_loss)/2\n        optimizer_d1.zero_grad()\n        d_loss.backward(retain_graph=True)\n        optimizer_d1.step()\n\n        # Train Generator\n        \n\n        g_loss = criterion_gan(discriminator_d1(fake_imgs), torch.ones(fake_imgs.size(0), 1, device=fake_imgs.device))\n        '''\n        pixel_loss = criterion_pixelwise(fake_imgs, real_images)\n        '''\n        pixel_loss = masked_l1_loss(fake_imgs, real_images, masks) #L1 loss counted only on the masked region\n        content_loss, style_loss = content_style_loss(fake_imgs, real_images)\n    \n        total_loss = g_loss + alpha * content_loss + 120 * style_loss + pixel_loss*6\n        \n        '''\n        hole_loss = F.l1_loss(fake_imgs * (1 - masks), real_images * (1 - masks), reduction='sum') / torch.sum(1 - masks)\n        valid_loss = F.l1_loss(fake_imgs * masks, real_images * masks, reduction='sum') / torch.sum(masks)\n        perceptual_loss_val = perceptual_loss(fake_imgs,real_images)\n        s_loss = style_loss(fake_imgs, real_images)\n        tv_loss = total_variation_loss(fake_imgs)\n        total_loss = valid_loss + 6 * hole_loss + 0.05 * perceptual_loss_val + 120 * s_loss + 0.1 * tv_loss\n        '''\n        \n        optimizer_g.zero_grad()\n        total_loss.backward()\n        optimizer_g.step()\n    \n        if i % save_interval == 0:\n            generator_path = f'generator_{epoch}_{i}.pth'\n            discriminator_path = f'discriminator_{epoch}_{i}.pth'\n            torch.save(generator.state_dict(), generator_path)\n            torch.save(discriminator_d1.state_dict(), discriminator_path)\n    # Update learning rate\n    generator.eval()\n    total_val_psnr = 0.0\n    total_val_ssim = 0.0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            real_images = batch['ground_truth'].to(device)\n            masked_images = batch['weighted_masked_image'].to(device)\n            masks = batch['mask'].to(device)\n\n            fake_images = generator(masked_images, masks)\n\n            # Normalize images if necessary\n            real_images = (real_images + 1) / 2\n            fake_images = (fake_images + 1) / 2\n\n            batch_psnr = calculate_psnr(real_images, fake_images)\n\n            total_val_psnr += batch_psnr\n\n    avg_val_psnr = total_val_psnr / len(val_dataloader)\n    print(f\"Epoch {epoch}: Avg. PSNR: {avg_val_psnr:.2f}\")\n\n    scheduler_g.step()\n    scheduler_d1.step()\n    print(f\"Epoch {epoch}/{num_epochs} - D1 Loss: {d_loss.item()} - G Loss: {total_loss.item()}\")\n    print(f\"valid loos {valid_loss.item()} - Hole Loss: {hole_loss.item()} - perceptual loss: {perceptual_loss_val.item()}- style loss:{s_loss.item()}- total variation loss:{tv_loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-17T16:10:07.346225Z","iopub.execute_input":"2024-02-17T16:10:07.346756Z","iopub.status.idle":"2024-02-17T16:10:30.147326Z","shell.execute_reply.started":"2024-02-17T16:10:07.346726Z","shell.execute_reply":"2024-02-17T16:10:30.145893Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1348\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[43m              \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_header\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTransfer-encoding\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1283\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Send a complete request to the server.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1283\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1329\u001b[0m, in \u001b[0;36mHTTPConnection._send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1328\u001b[0m     body \u001b[38;5;241m=\u001b[39m _encode(body, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbody\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1329\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1448\u001b[0m, in \u001b[0;36mHTTPSConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnect to a host on a given (SSL) port.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n","File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:942\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    941\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n\u001b[0;32m--> 942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;66;03m# Might fail in OSs that don't implement TCP_NODELAY\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:824\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    823\u001b[0m err \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSOCK_STREAM\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    825\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n","File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:955\u001b[0m, in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    954\u001b[0m addrlist \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 955\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_socket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetaddrinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfamily\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    956\u001b[0m     af, socktype, proto, canonname, sa \u001b[38;5;241m=\u001b[39m res\n","\u001b[0;31mgaierror\u001b[0m: [Errno -3] Temporary failure in name resolution","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[75], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m generator \u001b[38;5;241m=\u001b[39m UNet()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      2\u001b[0m discriminator_d1 \u001b[38;5;241m=\u001b[39m Discriminator()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 3\u001b[0m vgg16_feature_extractor \u001b[38;5;241m=\u001b[39m \u001b[43mVGG16FeatureExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Used within ContentStyleLoss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m content_style_loss \u001b[38;5;241m=\u001b[39m ContentStyleLoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m perceptual_loss \u001b[38;5;241m=\u001b[39m PerceptualLoss()\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Optional based on your preference\u001b[39;00m\n","Cell \u001b[0;32mIn[66], line 5\u001b[0m, in \u001b[0;36mVGG16FeatureExtractor.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m(VGG16FeatureExtractor, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 5\u001b[0m     vgg16 \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvgg16\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVGG16_Weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIMAGENET1K_V1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfeatures\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(vgg16\u001b[38;5;241m.\u001b[39mchildren())[:\u001b[38;5;241m23\u001b[39m])  \u001b[38;5;66;03m# Adjust based on the layers you need\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mparameters():\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:142\u001b[0m, in \u001b[0;36mkwonly_to_pos_or_kw.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequence_to_str(\u001b[38;5;28mtuple\u001b[39m(keyword_only_kwargs\u001b[38;5;241m.\u001b[39mkeys()),\u001b[38;5;250m \u001b[39mseparate_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as positional \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    140\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(keyword_only_kwargs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:228\u001b[0m, in \u001b[0;36mhandle_legacy_interface.<locals>.outer_wrapper.<locals>.inner_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[pretrained_param]\n\u001b[1;32m    226\u001b[0m     kwargs[weights_param] \u001b[38;5;241m=\u001b[39m default_weights_arg\n\u001b[0;32m--> 228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbuilder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/vgg.py:433\u001b[0m, in \u001b[0;36mvgg16\u001b[0;34m(weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"VGG-16 from `Very Deep Convolutional Networks for Large-Scale Image Recognition <https://arxiv.org/abs/1409.1556>`__.\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    :members:\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    431\u001b[0m weights \u001b[38;5;241m=\u001b[39m VGG16_Weights\u001b[38;5;241m.\u001b[39mverify(weights)\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_vgg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/vgg.py:105\u001b[0m, in \u001b[0;36m_vgg\u001b[0;34m(cfg, batch_norm, weights, progress, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m model \u001b[38;5;241m=\u001b[39m VGG(make_layers(cfgs[cfg], batch_norm\u001b[38;5;241m=\u001b[39mbatch_norm), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mweights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torchvision/models/_api.py:90\u001b[0m, in \u001b[0;36mWeightsEnum.get_state_dict\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_dict\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Mapping[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_state_dict_from_url\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/hub.py:766\u001b[0m, in \u001b[0;36mload_state_dict_from_url\u001b[0;34m(url, model_dir, map_location, progress, check_hash, file_name, weights_only)\u001b[0m\n\u001b[1;32m    764\u001b[0m         r \u001b[38;5;241m=\u001b[39m HASH_REGEX\u001b[38;5;241m.\u001b[39msearch(filename)  \u001b[38;5;66;03m# r is Optional[Match[str]]\u001b[39;00m\n\u001b[1;32m    765\u001b[0m         hash_prefix \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 766\u001b[0m     \u001b[43mdownload_url_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcached_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhash_prefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_legacy_zip_format(cached_file):\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_zip_load(cached_file, model_dir, map_location, weights_only)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/hub.py:620\u001b[0m, in \u001b[0;36mdownload_url_to_file\u001b[0;34m(url, dst, hash_prefix, progress)\u001b[0m\n\u001b[1;32m    618\u001b[0m file_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    619\u001b[0m req \u001b[38;5;241m=\u001b[39m Request(url, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.hub\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m--> 620\u001b[0m u \u001b[38;5;241m=\u001b[39m \u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m meta \u001b[38;5;241m=\u001b[39m u\u001b[38;5;241m.\u001b[39minfo()\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(meta, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgetheaders\u001b[39m\u001b[38;5;124m'\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:216\u001b[0m, in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    215\u001b[0m     opener \u001b[38;5;241m=\u001b[39m _opener\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[1;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[0;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[1;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[0;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[1;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[1;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[0;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[0;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/urllib/request.py:1351\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[0;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         h\u001b[38;5;241m.\u001b[39mrequest(req\u001b[38;5;241m.\u001b[39mget_method(), req\u001b[38;5;241m.\u001b[39mselector, req\u001b[38;5;241m.\u001b[39mdata, headers,\n\u001b[1;32m   1349\u001b[0m                   encode_chunked\u001b[38;5;241m=\u001b[39mreq\u001b[38;5;241m.\u001b[39mhas_header(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTransfer-encoding\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m-> 1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m   1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mgetresponse()\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n","\u001b[0;31mURLError\u001b[0m: <urlopen error [Errno -3] Temporary failure in name resolution>"],"ename":"URLError","evalue":"<urlopen error [Errno -3] Temporary failure in name resolution>","output_type":"error"}]}]}