{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2447212,"sourceType":"datasetVersion","datasetId":1479448},{"sourceId":6882730,"sourceType":"datasetVersion","datasetId":3954430},{"sourceId":6882761,"sourceType":"datasetVersion","datasetId":3954449}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torchvision.models as models\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision.models as models\nimport torch.nn as nn","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-12T19:08:02.158248Z","iopub.execute_input":"2024-02-12T19:08:02.158766Z","iopub.status.idle":"2024-02-12T19:08:09.796893Z","shell.execute_reply.started":"2024-02-12T19:08:02.158714Z","shell.execute_reply":"2024-02-12T19:08:09.795741Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install gdown","metadata":{"execution":{"iopub.status.busy":"2024-02-12T19:07:26.458793Z","iopub.execute_input":"2024-02-12T19:07:26.459611Z","iopub.status.idle":"2024-02-12T19:07:44.533110Z","shell.execute_reply.started":"2024-02-12T19:07:26.459565Z","shell.execute_reply":"2024-02-12T19:07:44.531708Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2023.11.17)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.1.0-py3-none-any.whl (17 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif device:\n    print('Model is running on GPU:', device)\nelse:\n    print('Model is running on CPU')\n\nclass InpaintingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None, subfolder_limit=10):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n\n        # Always initialize image_list and mask_list to ensure they are defined\n        self.image_list = []\n        self.mask_list = []\n\n        if subfolder_limit > 0:\n            self.subfolder_limit = subfolder_limit  # New parameter to limit subfolders\n\n            # Populate the lists with files from the first N subfolders\n            image_subfolders = sorted([os.path.join(image_dir, name) for name in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, name))])[:subfolder_limit]\n\n            for subfolder in image_subfolders:\n                for file in os.listdir(subfolder):\n                    if os.path.splitext(file)[1].lower() in ['.jpg', '.png', '.jpeg']:\n                        self.image_list.append(os.path.join(subfolder, file))\n        else:\n            self.image_list = [os.path.join(dp, f) for dp, dn, filenames in os.walk(image_dir) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg', '.png', '.jpeg']]\n\n        # Populate mask_list with all files from the mask directory\n        self.mask_list = [os.path.join(dp, f) for dp, dn, filenames in os.walk(mask_dir) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg', '.png', '.jpeg']]\n\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n\n    def __len__(self):\n        return len(self.image_list)\n    \n    def dilate_mask(self, mask, dilation_kernel_size=3):\n        kernel = np.ones((dilation_kernel_size, dilation_kernel_size), np.uint8)\n        dilated_mask = cv2.dilate(mask.numpy(), kernel, iterations=1)\n        return torch.from_numpy(dilated_mask)\n\n    def create_weight_map(self, mask, dilated_mask, border_weight=2.0):\n        border = dilated_mask - mask\n        weight_map = torch.ones_like(mask)\n        weight_map[border == 1] = border_weight\n        return weight_map  # Add this line\n\n\n    def __getitem__(self, idx):\n        image_path = self.image_list[idx]\n        # Randomly select a mask\n        mask_path = random.choice(self.mask_list)\n\n        image = Image.open(image_path).convert('RGB')\n        mask = Image.open(mask_path).convert('1')\n\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n\n        # Ensure mask is a binary tensor with the same size as image in the channel dimension\n        mask = mask.expand_as(image)\n\n        masked_image = image * (1 - mask)\n        \n        mask = (mask > 0).float()\n\n        masked_image = image * (1 - mask)\n\n        dilated_mask = self.dilate_mask(mask)\n        weight_map = self.create_weight_map(mask, dilated_mask)\n        \n        \n        \n        weighted_masked_image = masked_image * weight_map\n\n        return {\n            'ground_truth': image, \n            'weighted_masked_image': weighted_masked_image, \n            'mask': dilated_mask\n            }\n\nimage_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    \n])\n\n# Define your mask transformations including random rotation, flip, and dilation\nmask_transform = transforms.Compose([\n    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n])\n\n# Create dataset instances\ntrain_dataset = InpaintingDataset(\n    # image_dir='E:/1807033/Dataset/TEMPTRAINIMG',\n    # mask_dir='E:/1807033/Dataset/TEMPTRAINMASK',\n    image_dir='/kaggle/input/places2-mit-dataset/train_256_places365standard/data_256/a/',\n    mask_dir='/kaggle/input/training-mask',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n    subfolder_limit=10  # Only include the first 10 subfolders\n)\n\nval_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/validation-image',\n    mask_dir='/kaggle/input/validation-mask',\n    # image_dir='E:/1807033/Dataset/TEMPVALIMG',\n    # mask_dir='E:/1807033/Dataset/TEMPVALMASK',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n    subfolder_limit=0\n)\n\nfrom PIL import Image\nimport os\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class VGG16FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(VGG16FeatureExtractor, self).__init__()\n        vgg16 = models.vgg16(pretrained=True).features\n        self.features = nn.Sequential(*list(vgg16.children())[:23])  # Adjust based on the layers you need\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        return self.features(x)\n\nclass ContentStyleLoss(nn.Module):\n    def __init__(self):\n        super(ContentStyleLoss, self).__init__()\n        self.feature_extractor = VGG16FeatureExtractor()\n\n    def compute_gram_matrix(self, input):\n        a, b, c, d = input.size()  # a=batch size(=1)\n        features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n        G = torch.mm(features, features.t())  # compute the gram product\n        return G.div(a * b * c * d)\n\n    def forward(self, generated, target):\n        gen_features = self.feature_extractor(generated)\n        target_features = self.feature_extractor(target)\n        content_loss = F.mse_loss(gen_features, target_features)\n\n        # Compute style loss\n        gen_gram = self.compute_gram_matrix(gen_features)\n        target_gram = self.compute_gram_matrix(target_features)\n        style_loss = F.mse_loss(gen_gram, target_gram)\n\n        return content_loss, style_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass PerceptualLoss(nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n        vgg19 = models.vgg19(pretrained=True).features\n        self.vgg19 = nn.Sequential(*list(vgg19.children())[:36]).eval()  # Up to the second conv layer in the 5th block\n        for param in self.vgg19.parameters():\n            param.requires_grad = False\n\n    def forward(self, inpainted_image, target_image):\n        perception_loss = nn.MSELoss()\n        return perception_loss(self.vgg19(inpainted_image), self.vgg19(target_image))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            # Simple discriminator model\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, kernel_size=4, padding=0),  # Adjust the kernel size/padding for final size\n            nn.Sigmoid()\n        )\n        \n    def forward(self, img):\n        validity = self.model(img)\n        return validity","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_size),\n            nn.ReLU(inplace=True)\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.down1 = UNetDown(3, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x, mask):\n        # Encoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n\n        # Decoder\n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        inpainted = super().forward(u7)\n        \n        # Blend the inpainted output with the original image outside the masked region\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\n        output = (1 - mask) * image + mask * inpainted\n        return self.final(u7)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator = UNet()\ndiscriminator_d1 = DiscriminatorD1()\nvgg16_feature_extractor = VGG16FeatureExtractor()  # Used within ContentStyleLoss\ncontent_style_loss = ContentStyleLoss()\nperceptual_loss = PerceptualLoss()  # Optional based on your preference\n\n# Optimizers\noptimizer_g = optim.Adam(generator.parameters(), lr=3e-4)\noptimizer_d1 = optim.Adam(discriminator_d1.parameters(), lr=3e-4)\n\n# Learning rate scheduler for decay\nscheduler_g = optim.lr_scheduler.StepLR(optimizer_g, step_size=50, gamma=0.5)\nscheduler_d1 = optim.lr_scheduler.StepLR(optimizer_d1, step_size=50, gamma=0.5)\n\nfrom PIL import Image\nimport os\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Training Loop\nnum_epochs = 100\nalpha = 0.1  # Weight for content loss\nbeta = 0.2   # Weight for style loss\n\nfor epoch in range(num_epochs):\n    for i, batch in enumerate(train_dataloader):\n        if batch is None:\n            continue\n        real_images = batch['ground_truth'].to(device)\n        masked_images = batch['weighted_masked_image'].to(device)\n        masks = batch['mask'].to(device)\n        # Generate fake images\n        fake_imgs = generator(real_images, masks)\n\n        # ---------------------\n        #  Train Discriminator D1\n        # ---------------------\n        optimizer_d1.zero_grad()\n        \n        # Calculate loss for real and fake images\n        real_loss = criterion_gan(discriminator_d1(real_imgs), torch.ones_like(real_imgs))\n        fake_loss = criterion_gan(discriminator_d1(fake_imgs.detach()), torch.zeros_like(fake_imgs))\n        d1_loss = (real_loss + fake_loss) / 2\n\n        d1_loss.backward()\n        optimizer_d1.step()\n\n        # -----------------\n        #  Train Generator\n        # -----------------\n        optimizer_g.zero_grad()\n\n        # Adversarial loss\n        g_loss = criterion_gan(discriminator_d1(fake_imgs), torch.ones_like(fake_imgs))\n        \n        # Content and style loss\n        content_loss, style_loss = content_style_loss(fake_imgs, real_imgs)\n        total_loss = g_loss + alpha * content_loss + beta * style_loss\n\n        total_loss.backward()\n        optimizer_g.step()\n\n    # Update learning rate\n    scheduler_g.step()\n    scheduler_d1.step()\n\n    print(f\"Epoch {epoch}/{num_epochs} - D1 Loss: {d1_loss.item()} - G Loss: {total_loss.item()}\")","metadata":{},"execution_count":null,"outputs":[]}]}