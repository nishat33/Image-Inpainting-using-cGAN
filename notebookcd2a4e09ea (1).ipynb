{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6882730,"sourceType":"datasetVersion","datasetId":3954430},{"sourceId":6882761,"sourceType":"datasetVersion","datasetId":3954449},{"sourceId":6883197,"sourceType":"datasetVersion","datasetId":3954676},{"sourceId":6883247,"sourceType":"datasetVersion","datasetId":3954702},{"sourceId":7524754,"sourceType":"datasetVersion","datasetId":4383120}],"dockerImageVersionId":30646,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport tensorflow as tf\nfrom PIL import Image, ImageFile\nimport os\nimport random\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision.transforms.functional as TF\nimport random\nimport cv2\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-13T03:09:49.400864Z","iopub.execute_input":"2024-02-13T03:09:49.401648Z","iopub.status.idle":"2024-02-13T03:10:07.854566Z","shell.execute_reply.started":"2024-02-13T03:09:49.401618Z","shell.execute_reply":"2024-02-13T03:10:07.853459Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-02-13 03:09:57.933993: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-13 03:09:57.934122: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-13 03:09:58.071859: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim\n\ndef calculate_psnr(target, prediction, max_pixel=1.0):\n    mse = torch.mean((target - prediction) ** 2)\n    if mse == 0:\n        return float('inf')\n    return 20 * torch.log10(max_pixel / torch.sqrt(mse))\n\ndef calculate_ssim(target, prediction, data_range=1.0, channel_axis=-1):\n    # Convert tensors to numpy arrays\n    target_np = target.cpu().detach().numpy()\n    prediction_np = prediction.cpu().detach().numpy()\n    # Calculate SSIM over the batch\n    ssim_val = np.mean([ssim(t, p, data_range=data_range, channel_axis=channel_axis) for t, p in zip(target_np, prediction_np)])\n    return ssim_val","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:07.856750Z","iopub.execute_input":"2024-02-13T03:10:07.857516Z","iopub.status.idle":"2024-02-13T03:10:08.189673Z","shell.execute_reply.started":"2024-02-13T03:10:07.857473Z","shell.execute_reply":"2024-02-13T03:10:08.188582Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif device:\n    print('Model is running on GPU:', device)\nelse:\n    print('Model is running on CPU')\n\nclass InpaintingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None, subfolder_limit=10):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n\n        # Always initialize image_list and mask_list to ensure they are defined\n        self.image_list = []\n        self.mask_list = []\n\n        if subfolder_limit > 0:\n            self.subfolder_limit = subfolder_limit  # New parameter to limit subfolders\n\n            # Populate the lists with files from the first N subfolders\n            image_subfolders = sorted([os.path.join(image_dir, name) for name in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, name))])[:subfolder_limit]\n\n            for subfolder in image_subfolders:\n                for file in os.listdir(subfolder):\n                    if os.path.splitext(file)[1].lower() in ['.jpg', '.png', '.jpeg']:\n                        self.image_list.append(os.path.join(subfolder, file))\n        else:\n            self.image_list = [os.path.join(dp, f) for dp, dn, filenames in os.walk(image_dir) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg', '.png', '.jpeg']]\n\n        # Populate mask_list with all files from the mask directory\n        self.mask_list = [os.path.join(dp, f) for dp, dn, filenames in os.walk(mask_dir) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg', '.png', '.jpeg']]\n\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n\n    def __len__(self):\n        return len(self.image_list)\n    \n    def dilate_mask(self, mask, dilation_kernel_size=3):\n        kernel = np.ones((dilation_kernel_size, dilation_kernel_size), np.uint8)\n        dilated_mask = cv2.dilate(mask.numpy(), kernel, iterations=1)\n        return torch.from_numpy(dilated_mask)\n\n    def create_weight_map(self, mask, dilated_mask, border_weight=2.0):\n        border = dilated_mask - mask\n        weight_map = torch.ones_like(mask)\n        weight_map[border == 1] = border_weight\n        return weight_map  # Add this line\n\n\n    def __getitem__(self, idx):\n        image_path = self.image_list[idx]\n        # Randomly select a mask\n        mask_path = random.choice(self.mask_list)\n\n        image = Image.open(image_path).convert('RGB')\n        mask = Image.open(mask_path).convert('1')\n\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n\n        # Ensure mask is a binary tensor with the same size as image in the channel dimension\n        mask = mask.expand_as(image)\n\n        masked_image = image * (1 - mask)\n        \n        mask = (mask > 0).float()\n\n        masked_image = image * (1 - mask)\n\n        dilated_mask = self.dilate_mask(mask)\n        weight_map = self.create_weight_map(mask, dilated_mask)\n        \n        \n        \n        weighted_masked_image = masked_image * weight_map\n\n        return {\n            'ground_truth': image, \n            'weighted_masked_image': weighted_masked_image, \n            'mask': dilated_mask\n            }\n\nimage_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    \n])\n\n# Define your mask transformations including random rotation, flip, and dilation\nmask_transform = transforms.Compose([\n    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n])\n\n# Create dataset instances\ntrain_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/airplane',\n    mask_dir='/kaggle/input/training-mask',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n    subfolder_limit=0  # Only include the first 10 subfolders\n)\n\n'''val_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/validation-image',\n    mask_dir='/kaggle/input/validation-mask',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n    subfolder_limit=0\n)\n'''\nfrom PIL import Image\nimport os\nimport math\nfrom torch.utils.data import DataLoader, random_split\n\ntotal_size = len(train_dataset)\ntrain_size = math.floor(0.9 * total_size)\nval_size = total_size - train_size\n\n# Split the dataset\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:08.191396Z","iopub.execute_input":"2024-02-13T03:10:08.191985Z","iopub.status.idle":"2024-02-13T03:10:08.484669Z","shell.execute_reply.started":"2024-02-13T03:10:08.191958Z","shell.execute_reply":"2024-02-13T03:10:08.483381Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Model is running on GPU: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"class VGG16FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(VGG16FeatureExtractor, self).__init__()\n        vgg16 = torchvision.models.vgg16(pretrained=True).features\n        self.features = nn.Sequential(*list(vgg16.children())[:23])  # Adjust based on the layers you need\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        return self.features(x)\n\nclass ContentStyleLoss(nn.Module):\n    def __init__(self):\n        super(ContentStyleLoss, self).__init__()\n        self.feature_extractor = VGG16FeatureExtractor()\n\n    def compute_gram_matrix(self, input):\n        a, b, c, d = input.size()  # a=batch size(=1)\n        features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n        G = torch.mm(features, features.t())  # compute the gram product\n        return G.div(a * b * c * d)\n\n    def forward(self, generated, target):\n        gen_features = self.feature_extractor(generated)\n        target_features = self.feature_extractor(target)\n        content_loss = F.mse_loss(gen_features, target_features)\n\n        # Compute style loss\n        gen_gram = self.compute_gram_matrix(gen_features)\n        target_gram = self.compute_gram_matrix(target_features)\n        style_loss = F.mse_loss(gen_gram, target_gram)\n\n        return content_loss, style_loss","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:08.487453Z","iopub.execute_input":"2024-02-13T03:10:08.488364Z","iopub.status.idle":"2024-02-13T03:10:08.500955Z","shell.execute_reply.started":"2024-02-13T03:10:08.488336Z","shell.execute_reply":"2024-02-13T03:10:08.499346Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\nclass PerceptualLoss(nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n        vgg19 = torchvision.models.vgg19(pretrained=True).features\n        self.vgg19 = nn.Sequential(*list(vgg19.children())[:36]).eval()  # Up to the second conv layer in the 5th block\n        for param in self.vgg19.parameters():\n            param.requires_grad = False\n\n    def forward(self, inpainted_image, target_image):\n        perception_loss = nn.MSELoss()\n        return perception_loss(self.vgg19(inpainted_image), self.vgg19(target_image))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:08.504387Z","iopub.execute_input":"2024-02-13T03:10:08.505153Z","iopub.status.idle":"2024-02-13T03:10:08.519564Z","shell.execute_reply.started":"2024-02-13T03:10:08.505107Z","shell.execute_reply":"2024-02-13T03:10:08.518721Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n'''class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            # Simple discriminator model\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),  # Adjust the kernel size/padding for final size\n            nn.Flatten(),  # Flatten the output to [batch_size, 1]\n            nn.Sigmoid()\n        )\n        \n    def forward(self, img):\n        validity = self.model(img)\n        return validity'''","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:08.520645Z","iopub.execute_input":"2024-02-13T03:10:08.522098Z","iopub.status.idle":"2024-02-13T03:10:08.550864Z","shell.execute_reply.started":"2024-02-13T03:10:08.522055Z","shell.execute_reply":"2024-02-13T03:10:08.549715Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'class Discriminator(nn.Module):\\n    def __init__(self):\\n        super(Discriminator, self).__init__()\\n        self.model = nn.Sequential(\\n            # Simple discriminator model\\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\\n            nn.BatchNorm2d(128),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\\n            nn.BatchNorm2d(256),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\\n            nn.BatchNorm2d(512),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),  # Adjust the kernel size/padding for final size\\n            nn.Flatten(),  # Flatten the output to [batch_size, 1]\\n            nn.Sigmoid()\\n        )\\n        \\n    def forward(self, img):\\n        validity = self.model(img)\\n        return validity'"},"metadata":{}}]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            # Initial convolution layer\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Subsequent convolutional layers\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Adaptive pooling layer added to ensure the feature map is reduced to 1x1\n            nn.AdaptiveAvgPool2d(1),\n            \n            # Final convolutional layer to produce a single scalar output\n            nn.Conv2d(512, 1, kernel_size=1),\n            nn.Flatten(),  # Flatten the output to ensure it is a scalar\n            nn.Sigmoid()  # Sigmoid activation to obtain a probability\n        )\n        \n    def forward(self, img):\n        validity = self.model(img)\n        return validity\n","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:08.552156Z","iopub.execute_input":"2024-02-13T03:10:08.552450Z","iopub.status.idle":"2024-02-13T03:10:08.569320Z","shell.execute_reply.started":"2024-02-13T03:10:08.552426Z","shell.execute_reply":"2024-02-13T03:10:08.568371Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"\n\nclass UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_size),\n            nn.ReLU(inplace=True)\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.down1 = UNetDown(3, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x, mask):\n        # Encoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n\n        # Decoder\n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        inpainted = self.final(u7)\n        \n        # Blend the inpainted output with the original image outside the masked region\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\n        output = (1 - mask) * x + mask * inpainted\n        return self.final(u7)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:08.570483Z","iopub.execute_input":"2024-02-13T03:10:08.570791Z","iopub.status.idle":"2024-02-13T03:10:08.591369Z","shell.execute_reply.started":"2024-02-13T03:10:08.570760Z","shell.execute_reply":"2024-02-13T03:10:08.590420Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\ndef compute_gradient_penalty(D, real_samples, fake_samples):\n    \"\"\"Calculates the gradient penalty for a batch of real and fake samples.\"\"\"\n    alpha = torch.rand((real_samples.size(0), 1, 1, 1), device=real_samples.device)\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates = D(interpolates)\n    fake = torch.ones(d_interpolates.size(), device=real_samples.device, requires_grad=False)\n    \n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    \n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:08.592424Z","iopub.execute_input":"2024-02-13T03:10:08.592722Z","iopub.status.idle":"2024-02-13T03:10:08.605376Z","shell.execute_reply.started":"2024-02-13T03:10:08.592698Z","shell.execute_reply":"2024-02-13T03:10:08.604430Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def masked_l1_loss(output, target, mask):\n    \"\"\"\n    Calculate L1 loss only for the masked regions.\n    \n    Parameters:\n    - output: the output from the generator (inpainted image).\n    - target: the ground truth image.\n    - mask: the binary mask indicating the regions to inpaint (1 for missing regions).\n    \n    Returns:\n    - The L1 loss computed only for the masked regions.\n    \"\"\"\n    # Ensure the mask is in the correct format (same size as output/target and binary)\n    mask = mask.expand_as(target)  # Expanding the mask to match the target dimensions if needed\n    \n    # Calculate the difference only in the masked regions\n    difference = (output - target) * mask  # Apply mask to the difference\n    \n    # Calculate the L1 loss only for the masked regions\n    loss = torch.abs(difference).sum() / mask.sum()  # Normalize by the number of masked pixels\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:08.609053Z","iopub.execute_input":"2024-02-13T03:10:08.609348Z","iopub.status.idle":"2024-02-13T03:10:08.616557Z","shell.execute_reply.started":"2024-02-13T03:10:08.609325Z","shell.execute_reply":"2024-02-13T03:10:08.615764Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Following is the implementation from 9th paper of my folder","metadata":{}},{"cell_type":"code","source":"'''''from torchvision.models import vgg16\n\nvgg = vgg16(pretrained=True).features\nfor param in vgg.parameters():\n    param.requires_grad = False\nvgg= vgg.to(device)\ndef gram_matrix(input):\n    a, b, c, d = input.size()  \n    features = input.view(a * b, c * d)  \n    G = torch.mm(features, features.t())  \n    return G.div(a * b * c * d)\n\ndef style_loss(output, target):\n    loss = 0\n    for out_feat, tgt_feat in zip(vgg(output), vgg(target)):\n        out_gram = gram_matrix(out_feat)\n        tgt_gram = gram_matrix(tgt_feat)\n        loss += F.l1_loss(out_gram, tgt_gram)\n    return loss\n'''\ndef total_variation_loss(image):\n    loss = torch.sum(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \\\n           torch.sum(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n    return loss\n\n'''\nfor epoch in range(num_epochs):\n    for data in dataloader:\n        inputs, targets = data\n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        hole_loss = F.l1_loss(outputs * (1 - masks), targets * (1 - masks), reduction='sum') / torch.sum(1 - masks)\n        valid_loss = F.l1_loss(outputs * masks, targets * masks, reduction='sum') / torch.sum(masks)\n        perceptual_loss = F.l1_loss(vgg(outputs), vgg(targets))\n        s_loss = style_loss(outputs, targets)\n        tv_loss = total_variation_loss(outputs)\n        \n        total_loss = valid_loss + 6 * hole_loss + 0.05 * perceptual_loss + 120 * s_loss + 0.1 * tv_loss\n        \n        total_loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch}, Loss: {total_loss.item()}\")'''","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:08.617660Z","iopub.execute_input":"2024-02-13T03:10:08.617989Z","iopub.status.idle":"2024-02-13T03:10:08.633332Z","shell.execute_reply.started":"2024-02-13T03:10:08.617964Z","shell.execute_reply":"2024-02-13T03:10:08.632469Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'\\nfor epoch in range(num_epochs):\\n    for data in dataloader:\\n        inputs, targets = data\\n        optimizer.zero_grad()\\n        \\n        outputs = model(inputs)\\n        hole_loss = F.l1_loss(outputs * (1 - masks), targets * (1 - masks), reduction=\\'sum\\') / torch.sum(1 - masks)\\n        valid_loss = F.l1_loss(outputs * masks, targets * masks, reduction=\\'sum\\') / torch.sum(masks)\\n        perceptual_loss = F.l1_loss(vgg(outputs), vgg(targets))\\n        s_loss = style_loss(outputs, targets)\\n        tv_loss = total_variation_loss(outputs)\\n        \\n        total_loss = valid_loss + 6 * hole_loss + 0.05 * perceptual_loss + 120 * s_loss + 0.1 * tv_loss\\n        \\n        total_loss.backward()\\n        optimizer.step()\\n\\n    print(f\"Epoch {epoch}, Loss: {total_loss.item()}\")'"},"metadata":{}}]},{"cell_type":"code","source":"generator = UNet().to(device)\ndiscriminator_d1 = Discriminator().to(device)\nvgg16_feature_extractor = VGG16FeatureExtractor().to(device)  # Used within ContentStyleLoss\ncontent_style_loss = ContentStyleLoss().to(device)\nperceptual_loss = PerceptualLoss().to(device)  # Optional based on your preference\ncriterion_gan= nn.BCEWithLogitsLoss()\n\n# Optimizers\noptimizer_g = torch.optim.Adam(generator.parameters(), lr=3e-4)\noptimizer_d1 = torch.optim.Adam(discriminator_d1.parameters(), lr=3e-4)\n\n# Learning rate scheduler for decay\nscheduler_g = torch.optim.lr_scheduler.StepLR(optimizer_g, step_size=2, gamma=0.5)\nscheduler_d1 = torch.optim.lr_scheduler.StepLR(optimizer_d1, step_size=2, gamma=0.5)\n\nfrom PIL import Image\nimport os\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Training Loop\nnum_epochs = 100\nalpha = 0.1  # Weight for content loss\nbeta = 0.2   # Weight for style loss\nlambda_gp = 10\nsave_interval=100\ncriterion_pixelwise = nn.L1Loss()\nfor epoch in range(num_epochs):\n    generator.train()\n    total_train_loss = 0.0\n    for i, batch in enumerate(train_dataloader):\n        if batch is None:\n            continue\n        real_images = batch['ground_truth'].to(device)\n        masked_images = batch['weighted_masked_image'].to(device)\n        masks = batch['mask'].to(device)\n        # Generate fake images\n        fake_imgs = generator(real_images, masks)\n\n        # ---------------------\n        #  Train Discriminator D1\n        # ---------------------\n        #optimizer_d1.zero_grad()\n        \n       \n        #discriminator_d1.zero_grad()\n\n        real_loss = criterion_gan(discriminator_d1(real_images), torch.ones(real_images.size(0), 1, device=real_images.device))\n        fake_loss = criterion_gan(discriminator_d1(fake_imgs.detach()), torch.zeros(fake_imgs.size(0), 1, device=fake_imgs.device))\n\n        #gradient_penalty = compute_gradient_penalty(discriminator_d1, real_images, fake_imgs)\n        #d_loss = -(torch.mean(real_loss) - torch.mean(fake_loss)) + lambda_gp * gradient_penalty\n        d_loss=(real_loss+fake_loss)/2\n        optimizer_d1.zero_grad()\n        d_loss.backward(retain_graph=True)\n        optimizer_d1.step()\n\n        # Train Generator\n        \n\n        '''g_loss = criterion_gan(discriminator_d1(fake_imgs), torch.ones(fake_imgs.size(0), 1, device=fake_imgs.device))\n       ''' '''pixel_loss = criterion_pixelwise(fake_imgs, real_images)''''''\n        pixel_loss = masked_l1_loss(fake_imgs, real_images, masks) #L1 loss counted only on the masked region\n        content_loss, style_loss = content_style_loss(fake_imgs, real_images)\n    \n        total_loss = g_loss + alpha * content_loss + 120 * style_loss + pixel_loss*6\n        '''\n        hole_loss = F.l1_loss(fake_imgs * (1 - masks), real_images * (1 - masks), reduction='sum') / torch.sum(1 - masks)\n        valid_loss = F.l1_loss(fake_imgs * masks, real_images * masks, reduction='sum') / torch.sum(masks)\n        perceptual_loss_val = perceptual_loss(fake_imgs, real_images)\n        content_loss, style_loss = content_style_loss(fake_imgs, real_images)\n        tv_loss = total_variation_loss(fake_imgs)\n        total_loss = valid_loss + 6 * hole_loss + 0.05 * perceptual_loss_val + 120 * style_loss + 0.1 * tv_loss\n        \n        optimizer_g.zero_grad()\n        total_loss.backward()\n        optimizer_g.step()\n    \n        if i % save_interval == 0:\n            generator_path = f'generator_{epoch}_{i}.pth'\n            discriminator_path = f'discriminator_{epoch}_{i}.pth'\n            torch.save(generator.state_dict(), generator_path)\n            torch.save(discriminator_d1.state_dict(), discriminator_path)\n    # Update learning rate\n    generator.eval()\n    total_val_psnr = 0.0\n    total_val_ssim = 0.0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            real_images = batch['ground_truth'].to(device)\n            masked_images = batch['weighted_masked_image'].to(device)\n            masks = batch['mask'].to(device)\n\n            fake_images = generator(masked_images, masks)\n\n            # Normalize images if necessary\n            real_images = (real_images + 1) / 2\n            fake_images = (fake_images + 1) / 2\n\n            batch_psnr = calculate_psnr(real_images, fake_images)\n\n            total_val_psnr += batch_psnr\n\n    avg_val_psnr = total_val_psnr / len(val_dataloader)\n    print(f\"Epoch {epoch}: Avg. PSNR: {avg_val_psnr:.2f}\")\n    scheduler_g.step()\n    scheduler_d1.step()\n    print(f\"Epoch {epoch}/{num_epochs} - D1 Loss: {d_loss.item()} - G Loss: {total_loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T03:10:08.634929Z","iopub.execute_input":"2024-02-13T03:10:08.635295Z","iopub.status.idle":"2024-02-13T03:10:28.653183Z","shell.execute_reply.started":"2024-02-13T03:10:08.635261Z","shell.execute_reply":"2024-02-13T03:10:28.651736Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:03<00:00, 164MB/s]  \n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:03<00:00, 146MB/s]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[12], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39ml1_loss(fake_imgs \u001b[38;5;241m*\u001b[39m masks, real_images \u001b[38;5;241m*\u001b[39m masks, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(masks)\n\u001b[1;32m     72\u001b[0m perceptual_loss_val \u001b[38;5;241m=\u001b[39m perceptual_loss(fake_imgs, real_images)\n\u001b[0;32m---> 73\u001b[0m content_loss, style_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcontent_style_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfake_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m tv_loss \u001b[38;5;241m=\u001b[39m total_variation_loss(fake_imgs)\n\u001b[1;32m     75\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m valid_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m hole_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;241m*\u001b[39m perceptual_loss_val \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m120\u001b[39m \u001b[38;5;241m*\u001b[39m style_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m tv_loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[4], line 31\u001b[0m, in \u001b[0;36mContentStyleLoss.forward\u001b[0;34m(self, generated, target)\u001b[0m\n\u001b[1;32m     29\u001b[0m gen_gram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gram_matrix(gen_features)\n\u001b[1;32m     30\u001b[0m target_gram \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_gram_matrix(target_features)\n\u001b[0;32m---> 31\u001b[0m style_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_gram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_gram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content_loss, style_loss\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:3329\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3326\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m   3328\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbroadcast_tensors(\u001b[38;5;28minput\u001b[39m, target)\n\u001b[0;32m-> 3329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 944.12 MiB is free. Process 2630 has 14.97 GiB memory in use. Of the allocated memory 13.50 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 944.12 MiB is free. Process 2630 has 14.97 GiB memory in use. Of the allocated memory 13.50 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]}]}