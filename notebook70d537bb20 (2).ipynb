{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6882730,"sourceType":"datasetVersion","datasetId":3954430},{"sourceId":6882761,"sourceType":"datasetVersion","datasetId":3954449},{"sourceId":6883197,"sourceType":"datasetVersion","datasetId":3954676},{"sourceId":6883247,"sourceType":"datasetVersion","datasetId":3954702},{"sourceId":7524754,"sourceType":"datasetVersion","datasetId":4383120}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport tensorflow as tf\nfrom PIL import Image, ImageFile\nimport os\nimport random\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision.transforms.functional as TF\nimport random\nimport cv2\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-16T22:27:55.202104Z","iopub.execute_input":"2024-02-16T22:27:55.202565Z","iopub.status.idle":"2024-02-16T22:27:55.209414Z","shell.execute_reply.started":"2024-02-16T22:27:55.202526Z","shell.execute_reply":"2024-02-16T22:27:55.208509Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim\n\ndef calculate_psnr(target, prediction, max_pixel=1.0):\n    mse = torch.mean((target - prediction) ** 2)\n    if mse == 0:\n        return float('inf')\n    return 20 * torch.log10(max_pixel / torch.sqrt(mse))\n\ndef calculate_ssim(target, prediction, data_range=1.0, channel_axis=-1):\n    # Convert tensors to numpy arrays\n    target_np = target.cpu().detach().numpy()\n    prediction_np = prediction.cpu().detach().numpy()\n    # Calculate SSIM over the batch\n    ssim_val = np.mean([ssim(t, p, data_range=data_range, channel_axis=channel_axis) for t, p in zip(target_np, prediction_np)])\n    return ssim_val","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.211553Z","iopub.execute_input":"2024-02-16T22:27:55.212204Z","iopub.status.idle":"2024-02-16T22:27:55.220819Z","shell.execute_reply.started":"2024-02-16T22:27:55.212169Z","shell.execute_reply":"2024-02-16T22:27:55.220052Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif device:\n    print('Model is running on GPU:', device)\nelse:\n    print('Model is running on CPU')\n\nclass InpaintingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None, subfolder_limit=10):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n\n        # Always initialize image_list and mask_list to ensure they are defined\n        self.image_list = []\n        self.mask_list = []\n\n        if subfolder_limit > 0:\n            self.subfolder_limit = subfolder_limit  # New parameter to limit subfolders\n\n            # Populate the lists with files from the first N subfolders\n            image_subfolders = sorted([os.path.join(image_dir, name) for name in os.listdir(image_dir) if os.path.isdir(os.path.join(image_dir, name))])[:subfolder_limit]\n\n            for subfolder in image_subfolders:\n                for file in os.listdir(subfolder):\n                    if os.path.splitext(file)[1].lower() in ['.jpg', '.png', '.jpeg']:\n                        self.image_list.append(os.path.join(subfolder, file))\n        else:\n            self.image_list = [os.path.join(dp, f) for dp, dn, filenames in os.walk(image_dir) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg', '.png', '.jpeg']]\n\n        # Populate mask_list with all files from the mask directory\n        self.mask_list = [os.path.join(dp, f) for dp, dn, filenames in os.walk(mask_dir) for f in filenames if os.path.splitext(f)[1].lower() in ['.jpg', '.png', '.jpeg']]\n\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n\n    def __len__(self):\n        return len(self.image_list)\n    \n    def dilate_mask(self, mask, dilation_kernel_size=3):\n        kernel = np.ones((dilation_kernel_size, dilation_kernel_size), np.uint8)\n        dilated_mask = cv2.dilate(mask.numpy(), kernel, iterations=1)\n        return torch.from_numpy(dilated_mask)\n\n    def create_weight_map(self, mask, dilated_mask, border_weight=2.0):\n        border = dilated_mask - mask\n        weight_map = torch.ones_like(mask)\n        weight_map[border == 1] = border_weight\n        return weight_map  # Add this line\n\n\n    def __getitem__(self, idx):\n        image_path = self.image_list[idx]\n        # Randomly select a mask\n        mask_path = random.choice(self.mask_list)\n\n        image = Image.open(image_path).convert('RGB')\n        mask = Image.open(mask_path).convert('1')\n\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n\n        # Ensure mask is a binary tensor with the same size as image in the channel dimension\n        mask = mask.expand_as(image)\n\n        masked_image = image * (1 - mask)\n        \n        mask = (mask > 0).float()\n\n        masked_image = image * (1 - mask)\n\n        dilated_mask = self.dilate_mask(mask)\n        weight_map = self.create_weight_map(mask, dilated_mask)\n        \n        \n        \n        weighted_masked_image = masked_image * weight_map\n\n        return {\n            'ground_truth': image, \n            'weighted_masked_image': weighted_masked_image, \n            'mask': dilated_mask\n            }\n\nimage_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    \n])\n\n# Define your mask transformations including random rotation, flip, and dilation\nmask_transform = transforms.Compose([\n    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n])\n\n# Create dataset instances\ntrain_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/airplane',\n    mask_dir='/kaggle/input/training-mask',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n    subfolder_limit=0  # Only include the first 10 subfolders\n)\n\n'''val_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/validation-image',\n    mask_dir='/kaggle/input/validation-mask',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n    subfolder_limit=0\n)\n'''\nfrom PIL import Image\nimport os\nimport math\nfrom torch.utils.data import DataLoader, random_split\n\ntotal_size = len(train_dataset)\ntrain_size = math.floor(0.9 * total_size)\nval_size = total_size - train_size\n\n# Split the dataset\ntrain_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.222392Z","iopub.execute_input":"2024-02-16T22:27:55.222739Z","iopub.status.idle":"2024-02-16T22:27:55.262285Z","shell.execute_reply.started":"2024-02-16T22:27:55.222706Z","shell.execute_reply":"2024-02-16T22:27:55.261403Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":"Model is running on GPU: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"class VGG16FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(VGG16FeatureExtractor, self).__init__()\n        vgg16 = torchvision.models.vgg16(pretrained=True).features\n        self.features = nn.Sequential(*list(vgg16.children())[:23])  # Adjust based on the layers you need\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        return self.features(x)\n\nclass ContentStyleLoss(nn.Module):\n    def __init__(self):\n        super(ContentStyleLoss, self).__init__()\n        self.feature_extractor = VGG16FeatureExtractor()\n\n    def compute_gram_matrix(self, input):\n        a, b, c, d = input.size()  # a=batch size(=1)\n        features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n        G = torch.mm(features, features.t())  # compute the gram product\n        return G.div(a * b * c * d)\n\n    def forward(self, generated, target):\n        gen_features = self.feature_extractor(generated)\n        target_features = self.feature_extractor(target)\n        content_loss = F.mse_loss(gen_features, target_features)\n\n        # Compute style loss\n        gen_gram = self.compute_gram_matrix(gen_features)\n        target_gram = self.compute_gram_matrix(target_features)\n        style_loss = F.mse_loss(gen_gram, target_gram)\n\n        return content_loss, style_loss","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.263339Z","iopub.execute_input":"2024-02-16T22:27:55.263933Z","iopub.status.idle":"2024-02-16T22:27:55.274083Z","shell.execute_reply.started":"2024-02-16T22:27:55.263906Z","shell.execute_reply":"2024-02-16T22:27:55.273074Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"\nclass PerceptualLoss(nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n        vgg19 = torchvision.models.vgg19(pretrained=True).features\n        self.vgg19 = nn.Sequential(*list(vgg19.children())[:36]).eval()  # Up to the second conv layer in the 5th block\n        for param in self.vgg19.parameters():\n            param.requires_grad = False\n\n    def forward(self, inpainted_image, target_image):\n        perception_loss = nn.MSELoss()\n        return perception_loss(self.vgg19(inpainted_image), self.vgg19(target_image))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.388397Z","iopub.execute_input":"2024-02-16T22:27:55.389126Z","iopub.status.idle":"2024-02-16T22:27:55.395879Z","shell.execute_reply.started":"2024-02-16T22:27:55.389093Z","shell.execute_reply":"2024-02-16T22:27:55.394888Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"code","source":"\n'''class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            # Simple discriminator model\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),  # Adjust the kernel size/padding for final size\n            nn.Flatten(),  # Flatten the output to [batch_size, 1]\n            nn.Sigmoid()\n        )\n        \n    def forward(self, img):\n        validity = self.model(img)\n        return validity'''","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.398146Z","iopub.execute_input":"2024-02-16T22:27:55.398537Z","iopub.status.idle":"2024-02-16T22:27:55.406558Z","shell.execute_reply.started":"2024-02-16T22:27:55.398510Z","shell.execute_reply":"2024-02-16T22:27:55.405390Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"'class Discriminator(nn.Module):\\n    def __init__(self):\\n        super(Discriminator, self).__init__()\\n        self.model = nn.Sequential(\\n            # Simple discriminator model\\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\\n            nn.BatchNorm2d(128),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\\n            nn.BatchNorm2d(256),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\\n            nn.BatchNorm2d(512),\\n            nn.LeakyReLU(0.2, inplace=True),\\n            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),  # Adjust the kernel size/padding for final size\\n            nn.Flatten(),  # Flatten the output to [batch_size, 1]\\n            nn.Sigmoid()\\n        )\\n        \\n    def forward(self, img):\\n        validity = self.model(img)\\n        return validity'"},"metadata":{}}]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            # Initial convolution layer\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Subsequent convolutional layers\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Adaptive pooling layer added to ensure the feature map is reduced to 1x1\n            nn.AdaptiveAvgPool2d(1),\n            \n            # Final convolutional layer to produce a single scalar output\n            nn.Conv2d(512, 1, kernel_size=1),\n            nn.Flatten(),  # Flatten the output to ensure it is a scalar\n            nn.Sigmoid()  # Sigmoid activation to obtain a probability\n        )\n        \n    def forward(self, img):\n        validity = self.model(img)\n        return validity\n","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.407646Z","iopub.execute_input":"2024-02-16T22:27:55.407945Z","iopub.status.idle":"2024-02-16T22:27:55.420146Z","shell.execute_reply.started":"2024-02-16T22:27:55.407913Z","shell.execute_reply":"2024-02-16T22:27:55.419153Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"code","source":"\n'''\nclass UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_size),\n            nn.ReLU(inplace=True)\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.down1 = UNetDown(3, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x, mask):\n        # Encoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n\n        # Decoder\n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        inpainted = self.final(u7)\n        \n        # Blend the inpainted output with the original image outside the masked region\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\n        output = (1 - mask) * x + mask * inpainted\n        return self.final(u7)'''","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.421619Z","iopub.execute_input":"2024-02-16T22:27:55.422021Z","iopub.status.idle":"2024-02-16T22:27:55.435994Z","shell.execute_reply.started":"2024-02-16T22:27:55.421970Z","shell.execute_reply":"2024-02-16T22:27:55.435068Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"'\\nclass UNetDown(nn.Module):\\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\\n        super(UNetDown, self).__init__()\\n        layers = [nn.Conv2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False)]\\n        if normalize:\\n            layers.append(nn.BatchNorm2d(out_size))\\n        layers.append(nn.LeakyReLU(0.2))\\n        if dropout:\\n            layers.append(nn.Dropout(dropout))\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x):\\n        return self.model(x)\\n\\nclass UNetUp(nn.Module):\\n    def __init__(self, in_size, out_size, dropout=0.0):\\n        super(UNetUp, self).__init__()\\n        layers = [\\n            nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False),\\n            nn.BatchNorm2d(out_size),\\n            nn.ReLU(inplace=True)\\n        ]\\n        if dropout:\\n            layers.append(nn.Dropout(dropout))\\n        self.model = nn.Sequential(*layers)\\n\\n    def forward(self, x, skip_input):\\n        x = self.model(x)\\n        x = torch.cat((x, skip_input), 1)\\n        return x\\n\\nclass UNet(nn.Module):\\n    def __init__(self):\\n        super(UNet, self).__init__()\\n\\n        self.down1 = UNetDown(3, 64, normalize=False)\\n        self.down2 = UNetDown(64, 128)\\n        self.down3 = UNetDown(128, 256)\\n        self.down4 = UNetDown(256, 512, dropout=0.5)\\n        self.down5 = UNetDown(512, 512, dropout=0.5)\\n        self.down6 = UNetDown(512, 512, dropout=0.5)\\n        self.down7 = UNetDown(512, 512, dropout=0.5)\\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\\n\\n        self.up1 = UNetUp(512, 512, dropout=0.5)\\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\\n        self.up5 = UNetUp(1024, 256)\\n        self.up6 = UNetUp(512, 128)\\n        self.up7 = UNetUp(256, 64)\\n\\n        self.final = nn.Sequential(\\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\\n            nn.Tanh()\\n        )\\n\\n    def forward(self, x, mask):\\n        # Encoder\\n        d1 = self.down1(x)\\n        d2 = self.down2(d1)\\n        d3 = self.down3(d2)\\n        d4 = self.down4(d3)\\n        d5 = self.down5(d4)\\n        d6 = self.down6(d5)\\n        d7 = self.down7(d6)\\n        d8 = self.down8(d7)\\n\\n        # Decoder\\n        u1 = self.up1(d8, d7)\\n        u2 = self.up2(u1, d6)\\n        u3 = self.up3(u2, d5)\\n        u4 = self.up4(u3, d4)\\n        u5 = self.up5(u4, d3)\\n        u6 = self.up6(u5, d2)\\n        u7 = self.up7(u6, d1)\\n\\n        inpainted = self.final(u7)\\n        \\n        # Blend the inpainted output with the original image outside the masked region\\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\\n        output = (1 - mask) * x + mask * inpainted\\n        return self.final(u7)'"},"metadata":{}}]},{"cell_type":"code","source":"class PartialConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n        super(PartialConv2d, self).__init__()\n        self.input_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias)\n        self.mask_conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, dilation, groups, False)\n        self.mask_conv.weight.data.fill_(1.0)\n        for param in self.mask_conv.parameters():\n            param.requires_grad = False\n\n    def forward(self, x, mask):\n        output = self.input_conv(x * mask)\n        if self.input_conv.bias is not None:\n            output_bias = self.input_conv.bias.view(1, -1, 1, 1).expand_as(output)\n        else:\n            output_bias = torch.zeros_like(output)\n        \n        with torch.no_grad():\n            output_mask = self.mask_conv(mask)\n\n        mask_ratio = self.mask_conv.weight.data.sum() / output_mask.sum()\n        output = mask_ratio * output - output_bias * mask_ratio + output_bias\n        new_mask = torch.ones_like(output_mask)\n        new_mask[output_mask == 0] = 0\n\n        return output, new_mask\n","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.438800Z","iopub.execute_input":"2024-02-16T22:27:55.439555Z","iopub.status.idle":"2024-02-16T22:27:55.449914Z","shell.execute_reply.started":"2024-02-16T22:27:55.439527Z","shell.execute_reply":"2024-02-16T22:27:55.448802Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"class UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        self.pc = PartialConv2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False)\n        layers = []\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, mask):\n        x, mask = self.pc(x, mask)\n        for layer in self.model:\n            x = layer(x)\n        return x, mask\nclass UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        self.up = nn.ConvTranspose2d(in_size * 2, out_size, kernel_size=4, stride=2, padding=1, bias=False)\n        self.pc = PartialConv2d(out_size, out_size, kernel_size=3, stride=1, padding=1, bias=False)\n        layers = [\n            nn.BatchNorm2d(out_size),\n            nn.ReLU(inplace=True)\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input, mask, skip_mask):\n        x = self.up(x)\n        # Correctly concatenate upsampled x with skip_input BEFORE passing through PartialConv2d\n        x = torch.cat((x, skip_input), 1)  # Ensure this concatenation doubles the channels as expected\n        mask = F.interpolate(mask, scale_factor=2, mode='nearest')\n        x, mask = self.pc(x, mask) \n        for layer in self.model:\n            x = layer(x)\n        return x, mask\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.down1 = UNetDown(3, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x, mask):\n        # Encoder\n        d1, mask1 = self.down1(x,mask)\n        d2, mask2 = self.down2(d1, mask1)\n        d3, mask3 = self.down3(d2, mask2)\n        d4, mask4 = self.down4(d3, mask3)\n        d5, mask5 = self.down5(d4, mask4)\n        d6, mask6 = self.down6(d5, mask5)\n        d7, mask7 = self.down7(d6, mask6)\n        d8, mask8 = self.down8(d7, mask7)\n\n        # Decoder\n        # Decoder\n        u1, mask_up1 = self.up1(d8, d7, mask8, mask7)  # Corrected to use d8 and mask8 as the initial inputs\n        u2, mask_up2 = self.up2(u1, d7, mask_up1, mask7)  # Proceed with correct skip connections\n        u3, mask_up3 = self.up3(u2, d6, mask_up2, mask6)\n        u4, mask_up4 = self.up4(u3, d5, mask_up3, mask5)\n        u5, mask_up5 = self.up5(u4, d4, mask_up4, mask4)\n        u6, mask_up6 = self.up6(u5, d3, mask_up5, mask3)\n        u7, mask_up7 = self.up7(u6, d2, mask_up6, mask2)\n        final_output, final_mask = self.final(u7, mask_up7)\n\n        # Blend the inpainted output with the original image outside the masked region\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\n        output = (1 - mask) * x + mask * inpainted\n        return self.final(u7)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.451415Z","iopub.execute_input":"2024-02-16T22:27:55.451707Z","iopub.status.idle":"2024-02-16T22:27:55.475912Z","shell.execute_reply.started":"2024-02-16T22:27:55.451682Z","shell.execute_reply":"2024-02-16T22:27:55.475052Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"\ndef compute_gradient_penalty(D, real_samples, fake_samples):\n    \"\"\"Calculates the gradient penalty for a batch of real and fake samples.\"\"\"\n    alpha = torch.rand((real_samples.size(0), 1, 1, 1), device=real_samples.device)\n    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n    d_interpolates = D(interpolates)\n    fake = torch.ones(d_interpolates.size(), device=real_samples.device, requires_grad=False)\n    \n    gradients = torch.autograd.grad(\n        outputs=d_interpolates,\n        inputs=interpolates,\n        grad_outputs=fake,\n        create_graph=True,\n        retain_graph=True,\n        only_inputs=True,\n    )[0]\n    \n    gradients = gradients.view(gradients.size(0), -1)\n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n    return gradient_penalty","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.477087Z","iopub.execute_input":"2024-02-16T22:27:55.477376Z","iopub.status.idle":"2024-02-16T22:27:55.486967Z","shell.execute_reply.started":"2024-02-16T22:27:55.477350Z","shell.execute_reply":"2024-02-16T22:27:55.486132Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"code","source":"def masked_l1_loss(output, target, mask):\n    \"\"\"\n    Calculate L1 loss only for the masked regions.\n    \n    Parameters:\n    - output: the output from the generator (inpainted image).\n    - target: the ground truth image.\n    - mask: the binary mask indicating the regions to inpaint (1 for missing regions).\n    \n    Returns:\n    - The L1 loss computed only for the masked regions.\n    \"\"\"\n    # Ensure the mask is in the correct format (same size as output/target and binary)\n    mask = mask.expand_as(target)  # Expanding the mask to match the target dimensions if needed\n    \n    # Calculate the difference only in the masked regions\n    difference = (output - target) * mask  # Apply mask to the difference\n    \n    # Calculate the L1 loss only for the masked regions\n    loss = torch.abs(difference).sum() / mask.sum()  # Normalize by the number of masked pixels\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.488225Z","iopub.execute_input":"2024-02-16T22:27:55.488577Z","iopub.status.idle":"2024-02-16T22:27:55.499025Z","shell.execute_reply.started":"2024-02-16T22:27:55.488544Z","shell.execute_reply":"2024-02-16T22:27:55.498238Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"markdown","source":"Following is the implementation from 9th paper of my folder","metadata":{}},{"cell_type":"code","source":"from torchvision.models import vgg16\n\nvgg = vgg16(pretrained=True).features\nfor param in vgg.parameters():\n    param.requires_grad = False\nvgg= vgg.to(device)\ndef gram_matrix(input):\n    if input.dim() == 3:\n        input = input.unsqueeze(0)  # Add a batch dimension if missing\n    a, b, c, d = input.size()\n    features = input.view(a * b, c * d)\n    G = torch.mm(features, features.t())\n    return G.div(a * b * c * d)\n\ndef style_loss(output, target):\n    loss = 0\n    for out_feat, tgt_feat in zip(vgg(output), vgg(target)):\n        out_gram = gram_matrix(out_feat)\n        tgt_gram = gram_matrix(tgt_feat)\n        loss += F.l1_loss(out_gram, tgt_gram)\n    return loss\n\n'''def compute_gram_matrix(input):\n    a, b, c, d = input.size()  # a=batch size(=1)\n    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n    G = torch.mm(features, features.t())  # compute the gram product\n    return G.div(a * b * c * d)\n\ndef style_loss(generated, target):\n    gen_features = self.feature_extractor(generated)\n    target_features = self.feature_extractor(target)\n    content_loss = F.mse_loss(gen_features, target_features)\n\n    # Compute style loss\n    gen_gram = self.compute_gram_matrix(gen_features)\n    target_gram = self.compute_gram_matrix(target_features)\n    style_loss = F.mse_loss(gen_gram, target_gram)\n    \n    return style_loss'''\n\ndef total_variation_loss(img, weight=0.1):\n    \"\"\"\n    Compute the Total Variation Loss.\n    \n    Parameters:\n    - img: Tensor, the input image of shape (N, C, H, W) where\n      N is the batch size,\n      C is the number of channels,\n      H is the height, and\n      W is the width.\n    - weight: float, the weight of the total variation loss.\n    \n    Returns:\n    - loss: Tensor, the total variation loss.\n    \"\"\"\n    # Calculate the differences between adjacent pixels\n    pixel_diff_y = torch.abs(img[:, :, 1:] - img[:, :, :-1])\n    pixel_diff_x = torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1])\n\n    # Sum up the differences\n    sum_pixel_diff = torch.sum(pixel_diff_y) + torch.sum(pixel_diff_x)\n\n    # Average over the batch and apply weight\n    loss = weight * sum_pixel_diff / img.shape[0]\n    \n    return loss\n\n'''\nfor epoch in range(num_epochs):\n    for data in dataloader:\n        inputs, targets = data\n        optimizer.zero_grad()\n        \n        outputs = model(inputs)\n        hole_loss = F.l1_loss(outputs * (1 - masks), targets * (1 - masks), reduction='sum') / torch.sum(1 - masks)\n        valid_loss = F.l1_loss(outputs * masks, targets * masks, reduction='sum') / torch.sum(masks)\n        perceptual_loss = F.l1_loss(vgg(outputs), vgg(targets))\n        s_loss = style_loss(outputs, targets)\n        tv_loss = total_variation_loss(outputs)\n        \n        total_loss = valid_loss + 6 * hole_loss + 0.05 * perceptual_loss + 120 * s_loss + 0.1 * tv_loss\n        \n        total_loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch}, Loss: {total_loss.item()}\")'''","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:27:55.500389Z","iopub.execute_input":"2024-02-16T22:27:55.500961Z","iopub.status.idle":"2024-02-16T22:27:57.220170Z","shell.execute_reply.started":"2024-02-16T22:27:55.500933Z","shell.execute_reply":"2024-02-16T22:27:57.219235Z"},"trusted":true},"execution_count":113,"outputs":[{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"'\\nfor epoch in range(num_epochs):\\n    for data in dataloader:\\n        inputs, targets = data\\n        optimizer.zero_grad()\\n        \\n        outputs = model(inputs)\\n        hole_loss = F.l1_loss(outputs * (1 - masks), targets * (1 - masks), reduction=\\'sum\\') / torch.sum(1 - masks)\\n        valid_loss = F.l1_loss(outputs * masks, targets * masks, reduction=\\'sum\\') / torch.sum(masks)\\n        perceptual_loss = F.l1_loss(vgg(outputs), vgg(targets))\\n        s_loss = style_loss(outputs, targets)\\n        tv_loss = total_variation_loss(outputs)\\n        \\n        total_loss = valid_loss + 6 * hole_loss + 0.05 * perceptual_loss + 120 * s_loss + 0.1 * tv_loss\\n        \\n        total_loss.backward()\\n        optimizer.step()\\n\\n    print(f\"Epoch {epoch}, Loss: {total_loss.item()}\")'"},"metadata":{}}]},{"cell_type":"code","source":"generator = UNet().to(device)\ndiscriminator_d1 = Discriminator().to(device)\nvgg16_feature_extractor = VGG16FeatureExtractor().to(device)  # Used within ContentStyleLoss\ncontent_style_loss = ContentStyleLoss().to(device)\nperceptual_loss = PerceptualLoss().to(device)  # Optional based on your preference\ncriterion_gan= nn.BCEWithLogitsLoss()\n\n# Optimizers\noptimizer_g = torch.optim.Adam(generator.parameters(), lr=3e-4)\noptimizer_d1 = torch.optim.Adam(discriminator_d1.parameters(), lr=3e-4)\n\n# Learning rate scheduler for decay\nscheduler_g = torch.optim.lr_scheduler.StepLR(optimizer_g, step_size=2, gamma=0.5)\nscheduler_d1 = torch.optim.lr_scheduler.StepLR(optimizer_d1, step_size=2, gamma=0.5)\n\nfrom PIL import Image\nimport os\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Training Loop\nnum_epochs = 100\nalpha = 0.1  # Weight for content loss\nbeta = 0.2   # Weight for style loss\nlambda_gp = 10\nsave_interval=100\ncriterion_pixelwise = nn.L1Loss()\nfor epoch in range(num_epochs):\n    generator.train()\n    total_train_loss = 0.0\n    for i, batch in enumerate(train_dataloader):\n        if batch is None:\n            continue\n        real_images = batch['ground_truth'].to(device)\n        masked_images = batch['weighted_masked_image'].to(device)\n        masks = batch['mask'].to(device)\n        # Generate fake images\n        fake_imgs = generator(real_images, masks)\n\n        # ---------------------\n        #  Train Discriminator D1\n        # ---------------------\n        #optimizer_d1.zero_grad()\n        \n       \n        #discriminator_d1.zero_grad()\n\n        real_loss = criterion_gan(discriminator_d1(real_images), torch.ones(real_images.size(0), 1, device=real_images.device))\n        fake_loss = criterion_gan(discriminator_d1(fake_imgs.detach()), torch.zeros(fake_imgs.size(0), 1, device=fake_imgs.device))\n\n        #gradient_penalty = compute_gradient_penalty(discriminator_d1, real_images, fake_imgs)\n        #d_loss = -(torch.mean(real_loss) - torch.mean(fake_loss)) + lambda_gp * gradient_penalty\n        d_loss=(real_loss+fake_loss)/2\n        optimizer_d1.zero_grad()\n        d_loss.backward(retain_graph=True)\n        optimizer_d1.step()\n\n        # Train Generator\n        \n\n        '''g_loss = criterion_gan(discriminator_d1(fake_imgs), torch.ones(fake_imgs.size(0), 1, device=fake_imgs.device))\n       ''' '''pixel_loss = criterion_pixelwise(fake_imgs, real_images)''''''\n        pixel_loss = masked_l1_loss(fake_imgs, real_images, masks) #L1 loss counted only on the masked region\n        content_loss, style_loss = content_style_loss(fake_imgs, real_images)\n    \n        total_loss = g_loss + alpha * content_loss + 120 * style_loss + pixel_loss*6\n        '''\n        hole_loss = F.l1_loss(fake_imgs * (1 - masks), real_images * (1 - masks), reduction='sum') / torch.sum(1 - masks)\n        valid_loss = F.l1_loss(fake_imgs * masks, real_images * masks, reduction='sum') / torch.sum(masks)\n        perceptual_loss_val = perceptual_loss(fake_imgs,real_images)\n        s_loss = style_loss(fake_imgs, real_images)\n        tv_loss = total_variation_loss(fake_imgs)\n        total_loss = valid_loss + 6 * hole_loss + 0.05 * perceptual_loss_val + 120 * s_loss + 0.1 * tv_loss\n        \n        \n        optimizer_g.zero_grad()\n        total_loss.backward()\n        optimizer_g.step()\n    \n        if i % save_interval == 0:\n            generator_path = f'generator_{epoch}_{i}.pth'\n            discriminator_path = f'discriminator_{epoch}_{i}.pth'\n            torch.save(generator.state_dict(), generator_path)\n            torch.save(discriminator_d1.state_dict(), discriminator_path)\n    # Update learning rate\n    generator.eval()\n    total_val_psnr = 0.0\n    total_val_ssim = 0.0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            real_images = batch['ground_truth'].to(device)\n            masked_images = batch['weighted_masked_image'].to(device)\n            masks = batch['mask'].to(device)\n\n            fake_images = generator(masked_images, masks)\n\n            # Normalize images if necessary\n            real_images = (real_images + 1) / 2\n            fake_images = (fake_images + 1) / 2\n\n            batch_psnr = calculate_psnr(real_images, fake_images)\n\n            total_val_psnr += batch_psnr\n\n    avg_val_psnr = total_val_psnr / len(val_dataloader)\n    print(f\"Epoch {epoch}: Avg. PSNR: {avg_val_psnr:.2f}\")\n\n    scheduler_g.step()\n    scheduler_d1.step()\n    print(f\"Epoch {epoch}/{num_epochs} - D1 Loss: {d_loss.item()} - G Loss: {total_loss.item()}\")\n    print(f\"valid loos {valid_loss.item()} - Hole Loss: {hole_loss.item()} - perceptual loss: {perceptual_loss_val.item()}- style loss:{s_loss.item()}- total variation loss:{tv_loss.item()}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-02-16T22:28:24.646210Z","iopub.execute_input":"2024-02-16T22:28:24.646559Z","iopub.status.idle":"2024-02-16T22:28:32.003351Z","shell.execute_reply.started":"2024-02-16T22:28:24.646532Z","shell.execute_reply":"2024-02-16T22:28:32.001949Z"},"trusted":true},"execution_count":115,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[115], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m masks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Generate fake images\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m fake_imgs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#  Train Discriminator D1\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m#discriminator_d1.zero_grad()\u001b[39;00m\n\u001b[1;32m     50\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m criterion_gan(discriminator_d1(real_images), torch\u001b[38;5;241m.\u001b[39mones(real_images\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39mreal_images\u001b[38;5;241m.\u001b[39mdevice))\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[110], line 69\u001b[0m, in \u001b[0;36mUNet.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# Encoder\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     d1, mask1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown1(x,mask)\n\u001b[0;32m---> 69\u001b[0m     d2, mask2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdown2\u001b[49m\u001b[43m(\u001b[49m\u001b[43md1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     d3, mask3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown3(d2, mask2)\n\u001b[1;32m     71\u001b[0m     d4, mask4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown4(d3, mask3)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[110], line 14\u001b[0m, in \u001b[0;36mUNetDown.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m---> 14\u001b[0m     x, mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel:\n\u001b[1;32m     16\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[109], line 21\u001b[0m, in \u001b[0;36mPartialConv2d.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m     output_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_conv(mask)\n\u001b[1;32m     20\u001b[0m mask_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_conv\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m/\u001b[39m output_mask\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m---> 21\u001b[0m output \u001b[38;5;241m=\u001b[39m mask_ratio \u001b[38;5;241m*\u001b[39m output \u001b[38;5;241m-\u001b[39m \u001b[43moutput_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask_ratio\u001b[49m \u001b[38;5;241m+\u001b[39m output_bias\n\u001b[1;32m     22\u001b[0m new_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones_like(output_mask)\n\u001b[1;32m     23\u001b[0m new_mask[output_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 13.06 MiB is free. Process 3223 has 14.73 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 179.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 13.06 MiB is free. Process 3223 has 14.73 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 179.33 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"}]}]}