{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7625342,"sourceType":"datasetVersion","datasetId":4442249},{"sourceId":7625566,"sourceType":"datasetVersion","datasetId":4442422},{"sourceId":7639623,"sourceType":"datasetVersion","datasetId":4452417},{"sourceId":7641569,"sourceType":"datasetVersion","datasetId":4453704},{"sourceId":7644544,"sourceType":"datasetVersion","datasetId":4455863}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import datasets, transforms\nimport tensorflow as tf\nfrom PIL import Image, ImageFile\nimport os\nimport random\nimport numpy as np\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport torchvision\nfrom torch.utils.tensorboard import SummaryWriter\nimport torchvision.transforms.functional as TF\nimport random\nimport cv2\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-17T21:06:08.916355Z","iopub.execute_input":"2024-02-17T21:06:08.916758Z","iopub.status.idle":"2024-02-17T21:06:08.924030Z","shell.execute_reply.started":"2024-02-17T21:06:08.916730Z","shell.execute_reply":"2024-02-17T21:06:08.923010Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"!pip install torch\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:06:08.935719Z","iopub.execute_input":"2024-02-17T21:06:08.936308Z","iopub.status.idle":"2024-02-17T21:06:41.352298Z","shell.execute_reply.started":"2024-02-17T21:06:08.936277Z","shell.execute_reply":"2024-02-17T21:06:41.351265Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2023.12.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom skimage.metrics import structural_similarity as ssim\n\ndef calculate_psnr(target, prediction, max_pixel=1.0):\n    mse = torch.mean((target - prediction) ** 2)\n    if mse == 0:\n        return float('inf')\n    return 20 * torch.log10(max_pixel / torch.sqrt(mse))\n\ndef calculate_ssim(target, prediction, data_range=1.0, channel_axis=-1):\n    # Convert tensors to numpy arrays\n    target_np = target.cpu().detach().numpy()\n    prediction_np = prediction.cpu().detach().numpy()\n    # Calculate SSIM over the batch\n    ssim_val = np.mean([ssim(t, p, data_range=data_range, channel_axis=channel_axis) for t, p in zip(target_np, prediction_np)])\n    return ssim_val","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:06:41.354782Z","iopub.execute_input":"2024-02-17T21:06:41.355284Z","iopub.status.idle":"2024-02-17T21:06:41.365028Z","shell.execute_reply.started":"2024-02-17T21:06:41.355244Z","shell.execute_reply":"2024-02-17T21:06:41.364049Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif device:\n    print('Model is running on GPU:', device)\nelse:\n    print('Model is running on CPU')\n\nclass InpaintingDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, image_transform=None, mask_transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.image_list = os.listdir(image_dir)\n        self.mask_list = os.listdir(mask_dir)\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n\n        # Ensure the lists are sorted so they correspond\n        self.image_list.sort()\n        self.mask_list.sort()\n\n    def __len__(self):\n        return len(self.image_list)\n    \n    def dilate_mask(self, mask, dilation_kernel_size=3):\n        kernel = np.ones((dilation_kernel_size, dilation_kernel_size), np.uint8)\n        dilated_mask = cv2.dilate(mask.numpy(), kernel, iterations=1)\n        return torch.from_numpy(dilated_mask)\n\n    def create_weight_map(self, mask, dilated_mask, border_weight=2.0):\n        border = dilated_mask - mask\n        weight_map = torch.ones_like(mask)\n        weight_map[border == 1] = border_weight\n        return weight_map  # Add this line\n\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(self.image_dir, self.image_list[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n\n        image = Image.open(image_path).convert('RGB')\n        mask = Image.open(mask_path).convert('1')\n\n        if self.image_transform:\n            image = self.image_transform(image)\n        if self.mask_transform:\n            mask = self.mask_transform(mask)\n\n        # Ensure mask is a binary tensor with the same size as image in the channel dimension\n        mask = mask.expand_as(image)\n\n        masked_image = image * (1 - mask)\n        \n        mask = (mask > 0).float()\n\n        masked_image = image * (1 - mask)\n\n        dilated_mask = self.dilate_mask(mask)\n        weight_map = self.create_weight_map(mask, dilated_mask)\n        \n        \n        \n        weighted_masked_image = masked_image * weight_map\n\n        return {\n            'ground_truth': image, \n            'weighted_masked_image': weighted_masked_image, \n            'mask': dilated_mask\n            }\n\n\n\nimage_transform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    \n])\n\n# Define your mask transformations including random rotation, flip, and dilation\nmask_transform = transforms.Compose([\n    transforms.Resize((256, 256), interpolation=transforms.InterpolationMode.NEAREST),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(20),\n    transforms.ToTensor(),\n])\n\n# Create dataset instances\ntrain_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/trainingimage/new_subdataset',\n    mask_dir='/kaggle/input/mask-dataset/training',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n)\n\nval_dataset = InpaintingDataset(\n    image_dir='/kaggle/input/valimages/ValPlaces2',\n    mask_dir='/kaggle/input/validation-mask',\n    image_transform=image_transform,\n    mask_transform=mask_transform,\n)\n\nfrom PIL import Image\nimport os\nimport math\nfrom torch.utils.data import DataLoader, random_split\n\ntotal_size = len(train_dataset)\n\n\n\n# Create data loaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:06:41.366402Z","iopub.execute_input":"2024-02-17T21:06:41.366912Z","iopub.status.idle":"2024-02-17T21:06:41.480418Z","shell.execute_reply.started":"2024-02-17T21:06:41.366881Z","shell.execute_reply":"2024-02-17T21:06:41.479701Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Model is running on GPU: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"from torchvision.models import vgg16, inception_v3, VGG16_Weights, Inception_V3_Weights\n\nweights_path = '/kaggle/input/vgg16model/vgg16-397923af.pth'\n\nmodel = vgg16()\nmodel.load_state_dict(torch.load(weights_path))\nmodel.eval()\n\nclass VGG16FeatureExtractor(nn.Module):\n    def __init__(self, weights_path):\n        super(VGG16FeatureExtractor, self).__init__()\n        # Initialize VGG16 model\n        vgg16_model = vgg16()\n        # Load the pretrained weights manually\n        vgg16_model.load_state_dict(torch.load(weights_path))\n        # Extract the features portion of VGG16\n        self.features = vgg16_model.features[:23]  # Adjust based on the layers you need\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        return self.features(x)\n\n# Path to the manually downloaded weights\nweights_path = '/kaggle/input/vgg16model/vgg16-397923af.pth'\n\n# Correct instantiation of VGG16FeatureExtractor\nvgg16_feature_extractor = VGG16FeatureExtractor(weights_path=weights_path).to(device)\n\n# Updated ContentStyleLoss class initialization to accept weights_path\nclass ContentStyleLoss(nn.Module):\n    def __init__(self, weights_path):\n        super(ContentStyleLoss, self).__init__()\n        self.feature_extractor = VGG16FeatureExtractor(weights_path=weights_path)\n\n    def compute_gram_matrix(self, input):\n        a, b, c, d = input.size()  # a=batch size (=1 for simplicity)\n        features = input.view(a * b, c * d)  # resize F_XL into \\hat F_XL\n        G = torch.mm(features, features.t())  # compute the gram product\n        return G.div(a * b * c * d)\n\n    def forward(self, generated, target):\n        gen_features = self.feature_extractor(generated)\n        target_features = self.feature_extractor(target)\n        content_loss = F.mse_loss(gen_features, target_features)\n\n        # Compute style loss\n        gen_gram = self.compute_gram_matrix(gen_features)\n        target_gram = self.compute_gram_matrix(target_features)\n        style_loss = F.mse_loss(gen_gram, target_gram)\n\n        return content_loss, style_loss\n\n# Correct instantiation of ContentStyleLoss with weights_path\ncontent_style_loss = ContentStyleLoss(weights_path=weights_path).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:06:41.482973Z","iopub.execute_input":"2024-02-17T21:06:41.483387Z","iopub.status.idle":"2024-02-17T21:06:49.822170Z","shell.execute_reply.started":"2024-02-17T21:06:41.483355Z","shell.execute_reply":"2024-02-17T21:06:49.821334Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"\nclass PerceptualLoss(nn.Module):\n    def __init__(self):\n        super(PerceptualLoss, self).__init__()\n        vgg16_model = vgg16()\n        # Load the pretrained weights manually\n        vgg16_model.load_state_dict(torch.load(weights_path))\n        # Extract the features portion of VGG16\n        self.features = vgg16_model.features[:23]  # Adjust based on the layers you need\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, inpainted_image, target_image):\n        perception_loss = nn.MSELoss()\n        return perception_loss(self.vgg19(inpainted_image), self.vgg19(target_image))\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:06:49.823469Z","iopub.execute_input":"2024-02-17T21:06:49.824108Z","iopub.status.idle":"2024-02-17T21:06:49.830974Z","shell.execute_reply.started":"2024-02-17T21:06:49.824074Z","shell.execute_reply":"2024-02-17T21:06:49.830062Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.model = nn.Sequential(\n            # Initial convolution layer\n            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Subsequent convolutional layers\n            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(128),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(256),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(512),\n            nn.LeakyReLU(0.2, inplace=True),\n            \n            # Adaptive pooling layer added to ensure the feature map is reduced to 1x1\n            nn.AdaptiveAvgPool2d(1),\n            \n            # Final convolutional layer to produce a single scalar output\n            nn.Conv2d(512, 1, kernel_size=1),\n            nn.Flatten(),  # Flatten the output to ensure it is a scalar\n            nn.Sigmoid()  # Sigmoid activation to obtain a probability\n        )\n        \n    def forward(self, img):\n        validity = self.model(img)\n        return validity\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:06:49.832309Z","iopub.execute_input":"2024-02-17T21:06:49.832650Z","iopub.status.idle":"2024-02-17T21:06:49.847256Z","shell.execute_reply.started":"2024-02-17T21:06:49.832624Z","shell.execute_reply":"2024-02-17T21:06:49.846229Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"\n\nclass UNetDown(nn.Module):\n    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n        super(UNetDown, self).__init__()\n        layers = [nn.Conv2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False)]\n        if normalize:\n            layers.append(nn.BatchNorm2d(out_size))\n        layers.append(nn.LeakyReLU(0.2))\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.model(x)\n\nclass UNetUp(nn.Module):\n    def __init__(self, in_size, out_size, dropout=0.0):\n        super(UNetUp, self).__init__()\n        layers = [\n            nn.ConvTranspose2d(in_size, out_size, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(out_size),\n            nn.ReLU(inplace=True)\n        ]\n        if dropout:\n            layers.append(nn.Dropout(dropout))\n        self.model = nn.Sequential(*layers)\n\n    def forward(self, x, skip_input):\n        x = self.model(x)\n        x = torch.cat((x, skip_input), 1)\n        return x\n\nclass UNet(nn.Module):\n    def __init__(self):\n        super(UNet, self).__init__()\n\n        self.down1 = UNetDown(3, 64, normalize=False)\n        self.down2 = UNetDown(64, 128)\n        self.down3 = UNetDown(128, 256)\n        self.down4 = UNetDown(256, 512, dropout=0.5)\n        self.down5 = UNetDown(512, 512, dropout=0.5)\n        self.down6 = UNetDown(512, 512, dropout=0.5)\n        self.down7 = UNetDown(512, 512, dropout=0.5)\n        self.down8 = UNetDown(512, 512, normalize=False, dropout=0.5)\n\n        self.up1 = UNetUp(512, 512, dropout=0.5)\n        self.up2 = UNetUp(1024, 512, dropout=0.5)\n        self.up3 = UNetUp(1024, 512, dropout=0.5)\n        self.up4 = UNetUp(1024, 512, dropout=0.5)\n        self.up5 = UNetUp(1024, 256)\n        self.up6 = UNetUp(512, 128)\n        self.up7 = UNetUp(256, 64)\n\n        self.final = nn.Sequential(\n            nn.ConvTranspose2d(128, 3, kernel_size=4, stride=2, padding=1),\n            nn.Tanh()\n        )\n\n    def forward(self, x, mask):\n        # Encoder\n        d1 = self.down1(x)\n        d2 = self.down2(d1)\n        d3 = self.down3(d2)\n        d4 = self.down4(d3)\n        d5 = self.down5(d4)\n        d6 = self.down6(d5)\n        d7 = self.down7(d6)\n        d8 = self.down8(d7)\n\n        # Decoder\n        u1 = self.up1(d8, d7)\n        u2 = self.up2(u1, d6)\n        u3 = self.up3(u2, d5)\n        u4 = self.up4(u3, d4)\n        u5 = self.up5(u4, d3)\n        u6 = self.up6(u5, d2)\n        u7 = self.up7(u6, d1)\n\n        inpainted = self.final(u7)\n        \n        # Blend the inpainted output with the original image outside the masked region\n        # This assumes mask is 1 for regions to inpaint and 0 elsewhere\n        output = (1 - mask) * x + mask * inpainted\n        return self.final(u7)","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:06:49.848838Z","iopub.execute_input":"2024-02-17T21:06:49.849229Z","iopub.status.idle":"2024-02-17T21:06:49.873542Z","shell.execute_reply.started":"2024-02-17T21:06:49.849198Z","shell.execute_reply":"2024-02-17T21:06:49.872608Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def masked_l1_loss(output, target, mask):\n    \"\"\"\n    Calculate L1 loss only for the masked regions.\n    \n    Parameters:\n    - output: the output from the generator (inpainted image).\n    - target: the ground truth image.\n    - mask: the binary mask indicating the regions to inpaint (1 for missing regions).\n    \n    Returns:\n    - The L1 loss computed only for the masked regions.\n    \"\"\"\n    # Ensure the mask is in the correct format (same size as output/target and binary)\n    mask = mask.expand_as(target)  # Expanding the mask to match the target dimensions if needed\n    \n    # Calculate the difference only in the masked regions\n    difference = (output - target) * mask  # Apply mask to the difference\n    \n    # Calculate the L1 loss only for the masked regions\n    loss = torch.abs(difference).sum() / mask.sum()  # Normalize by the number of masked pixels\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:06:49.910845Z","iopub.execute_input":"2024-02-17T21:06:49.911113Z","iopub.status.idle":"2024-02-17T21:06:49.921445Z","shell.execute_reply.started":"2024-02-17T21:06:49.911092Z","shell.execute_reply":"2024-02-17T21:06:49.920620Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Following is the implementation from 9th paper of my folder","metadata":{}},{"cell_type":"code","source":"generator = UNet().to(device)\ndiscriminator_d1 = Discriminator().to(device)\nvgg16_feature_extractor = VGG16FeatureExtractor(weights_path).to(device)  # Used within ContentStyleLoss\nperceptual_loss = PerceptualLoss().to(device)  # Optional based on your preference\ncriterion_gan= nn.BCEWithLogitsLoss()\n\n# Optimizers\noptimizer_g = torch.optim.Adam(generator.parameters(), lr=3e-4)\noptimizer_d1 = torch.optim.Adam(discriminator_d1.parameters(), lr=3e-4)\n\n# Learning rate scheduler for decay\nscheduler_g = torch.optim.lr_scheduler.StepLR(optimizer_g, step_size=2, gamma=0.5)\nscheduler_d1 = torch.optim.lr_scheduler.StepLR(optimizer_d1, step_size=2, gamma=0.5)\n\nfrom PIL import Image\nimport os\n\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\n# Training Loop\nnum_epochs = 100\nalpha = 0.1  # Weight for content loss\nbeta = 0.2   # Weight for style loss\nlambda_gp = 10\nsave_interval=100\ncriterion_pixelwise = nn.L1Loss()\nfor epoch in range(num_epochs):\n    generator.train()\n    total_train_loss = 0.0\n    for i, batch in enumerate(train_dataloader):\n        if batch is None:\n            continue\n        real_images = batch['ground_truth'].to(device)\n        masked_images = batch['weighted_masked_image'].to(device)\n        masks = batch['mask'].to(device)\n        # Generate fake images\n        fake_imgs = generator(real_images, masks)\n\n        # ---------------------\n        #  Train Discriminator D1\n        # ---------------------\n        #optimizer_d1.zero_grad()\n        \n       \n        #discriminator_d1.zero_grad()\n\n        real_loss = criterion_gan(discriminator_d1(real_images), torch.ones(real_images.size(0), 1, device=real_images.device))\n        fake_loss = criterion_gan(discriminator_d1(fake_imgs.detach()), torch.zeros(fake_imgs.size(0), 1, device=fake_imgs.device))\n\n        #gradient_penalty = compute_gradient_penalty(discriminator_d1, real_images, fake_imgs)\n        #d_loss = -(torch.mean(real_loss) - torch.mean(fake_loss)) + lambda_gp * gradient_penalty\n        d_loss=(real_loss+fake_loss)/2\n        optimizer_d1.zero_grad()\n        d_loss.backward(retain_graph=True)\n        optimizer_d1.step()\n\n        # Train Generator\n        \n\n        g_loss = criterion_gan(discriminator_d1(fake_imgs), torch.ones(fake_imgs.size(0), 1, device=fake_imgs.device))\n        '''\n        pixel_loss = criterion_pixelwise(fake_imgs, real_images)\n        '''\n        pixel_loss = masked_l1_loss(fake_imgs, real_images, masks) #L1 loss counted only on the masked region\n        content_loss, style_loss = content_style_loss(fake_imgs, real_images)\n    \n        total_loss = g_loss + alpha * content_loss + 120 * style_loss + pixel_loss*6\n        \n        '''\n        hole_loss = F.l1_loss(fake_imgs * (1 - masks), real_images * (1 - masks), reduction='sum') / torch.sum(1 - masks)\n        valid_loss = F.l1_loss(fake_imgs * masks, real_images * masks, reduction='sum') / torch.sum(masks)\n        perceptual_loss_val = perceptual_loss(fake_imgs,real_images)\n        s_loss = style_loss(fake_imgs, real_images)\n        tv_loss = total_variation_loss(fake_imgs)\n        total_loss = valid_loss + 6 * hole_loss + 0.05 * perceptual_loss_val + 120 * s_loss + 0.1 * tv_loss\n        '''\n        \n        optimizer_g.zero_grad()\n        total_loss.backward()\n        optimizer_g.step()\n    \n        if i % save_interval == 0:\n            generator_path = f'generator_{epoch}_{i}.pth'\n            discriminator_path = f'discriminator_{epoch}_{i}.pth'\n            torch.save(generator.state_dict(), generator_path)\n            torch.save(discriminator_d1.state_dict(), discriminator_path)\n            print(f\"Epoch {epoch}/{num_epochs} - D1 Loss: {d_loss.item()} - G Loss: {total_loss.item()}\")\n            print(f\"content loss {content_loss.item()}- style loss:{style_loss.item()}- pixel loss:{pixel_loss.item()}\")\n\n    # Update learning rate\n    generator.eval()\n    total_val_psnr = 0.0\n    total_val_ssim = 0.0\n    with torch.no_grad():\n        for batch in val_dataloader:\n            real_images = batch['ground_truth'].to(device)\n            masked_images = batch['weighted_masked_image'].to(device)\n            masks = batch['mask'].to(device)\n\n            fake_images = generator(masked_images, masks)\n\n            # Normalize images if necessary\n            real_images = (real_images + 1) / 2\n            fake_images = (fake_images + 1) / 2\n\n            batch_psnr = calculate_psnr(real_images, fake_images)\n\n            total_val_psnr += batch_psnr\n\n    avg_val_psnr = total_val_psnr / len(val_dataloader)\n    print(f\"Epoch {epoch}: Avg. PSNR: {avg_val_psnr:.2f}\")\n\n    scheduler_g.step()\n    scheduler_d1.step()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-17T21:06:49.936008Z","iopub.execute_input":"2024-02-17T21:06:49.936311Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 0/100 - D1 Loss: 0.7175222635269165 - G Loss: 7.770138263702393\ncontent loss 3.018120765686035- style loss:5.554732229740011e-10- pixel loss:1.16059410572052\nEpoch 0/100 - D1 Loss: 0.6471681594848633 - G Loss: 3.2672367095947266\ncontent loss 2.0315322875976562- style loss:2.4448687518940915e-10- pixel loss:0.4016691744327545\nEpoch 0/100 - D1 Loss: 0.5911558270454407 - G Loss: 2.9562466144561768\ncontent loss 1.574082851409912- style loss:1.656433878949315e-10- pixel loss:0.35414987802505493\nEpoch 0/100 - D1 Loss: 0.571150541305542 - G Loss: 2.594189405441284\ncontent loss 1.5426671504974365- style loss:1.6329632090972268e-10- pixel loss:0.29423028230667114\nEpoch 0/100 - D1 Loss: 0.5574823617935181 - G Loss: 2.7645344734191895\ncontent loss 1.573697566986084- style loss:2.1059341770435225e-10- pixel loss:0.3210901916027069\nEpoch 0/100 - D1 Loss: 0.5436769723892212 - G Loss: 2.841148614883423\ncontent loss 1.2881664037704468- style loss:1.4055523411826698e-10- pixel loss:0.3380291759967804\nEpoch 0/100 - D1 Loss: 0.5707178711891174 - G Loss: 2.4517223834991455\ncontent loss 1.4152429103851318- style loss:1.6292256432848262e-10- pixel loss:0.2707569897174835\nEpoch 0/100 - D1 Loss: 0.5469121932983398 - G Loss: 3.259859561920166\ncontent loss 1.344235897064209- style loss:1.522047210489319e-10- pixel loss:0.4065553843975067\nEpoch 0/100 - D1 Loss: 0.5329354405403137 - G Loss: 2.9772348403930664\ncontent loss 1.6850743293762207- style loss:2.2961479939631602e-10- pixel loss:0.35446596145629883\nEpoch 0/100 - D1 Loss: 0.53928142786026 - G Loss: 2.765035629272461\ncontent loss 1.3823344707489014- style loss:1.4714236773460954e-10- pixel loss:0.3226722478866577\nEpoch 0/100 - D1 Loss: 0.5430883169174194 - G Loss: 2.6346278190612793\ncontent loss 1.205264925956726- style loss:1.2169974750175783e-10- pixel loss:0.3039490282535553\nEpoch 0/100 - D1 Loss: 0.5523531436920166 - G Loss: 2.9197826385498047\ncontent loss 1.5319584608078003- style loss:2.1577571673869755e-10- pixel loss:0.3464861810207367\nEpoch 0/100 - D1 Loss: 0.5427953004837036 - G Loss: 2.5015206336975098\ncontent loss 1.424905776977539- style loss:1.688247597275705e-10- pixel loss:0.2784247398376465\nEpoch 0/100 - D1 Loss: 0.5208444595336914 - G Loss: 2.6733596324920654\ncontent loss 1.468506097793579- style loss:1.7113817307734536e-10- pixel loss:0.30635228753089905\nEpoch 0/100 - D1 Loss: 0.5200949907302856 - G Loss: 2.6172337532043457\ncontent loss 1.4205348491668701- style loss:1.6549969727996938e-10- pixel loss:0.2978155016899109\nEpoch 0/100 - D1 Loss: 0.5096259713172913 - G Loss: 2.6488237380981445\ncontent loss 1.4840880632400513- style loss:1.734401788855422e-10- pixel loss:0.3020678758621216\nEpoch 0: Avg. PSNR: 9.94\nEpoch 1/100 - D1 Loss: 0.5263105034828186 - G Loss: 2.8520984649658203\ncontent loss 1.5299315452575684- style loss:1.830929713397822e-10- pixel loss:0.3361114263534546\nEpoch 1/100 - D1 Loss: 0.5062718391418457 - G Loss: 2.82792067527771\ncontent loss 1.2387301921844482- style loss:1.3628806416754458e-10- pixel loss:0.33546313643455505\nEpoch 1/100 - D1 Loss: 0.5048441290855408 - G Loss: 2.6909267902374268\ncontent loss 1.4334123134613037- style loss:1.7347513703303008e-10- pixel loss:0.30924877524375916\nEpoch 1/100 - D1 Loss: 0.5046722292900085 - G Loss: 2.4892194271087646\ncontent loss 1.5815694332122803- style loss:2.0739503170386087e-10- pixel loss:0.27305519580841064\nEpoch 1/100 - D1 Loss: 0.504662811756134 - G Loss: 2.604142189025879\ncontent loss 1.3694030046463013- style loss:1.535627597304412e-10- pixel loss:0.29592594504356384\nEpoch 1/100 - D1 Loss: 0.5037660598754883 - G Loss: 2.6969552040100098\ncontent loss 1.5308926105499268- style loss:1.7901310989110186e-10- pixel loss:0.3085245192050934\nEpoch 1/100 - D1 Loss: 0.5040581226348877 - G Loss: 2.3827784061431885\ncontent loss 1.3439528942108154- style loss:1.5252524243614118e-10- pixel loss:0.2593430280685425\nEpoch 1/100 - D1 Loss: 0.503765344619751 - G Loss: 2.760528326034546\ncontent loss 1.3335493803024292- style loss:1.5193608871033604e-10- pixel loss:0.32244306802749634\nEpoch 1/100 - D1 Loss: 0.5037447810173035 - G Loss: 3.29512095451355\ncontent loss 1.3589239120483398- style loss:1.4936710202029246e-10- pixel loss:0.4110662043094635\nEpoch 1/100 - D1 Loss: 0.5036280155181885 - G Loss: 2.623542308807373\ncontent loss 1.3104212284088135- style loss:1.5756676519096402e-10- pixel loss:0.2999645471572876\nEpoch 1/100 - D1 Loss: 0.5036087036132812 - G Loss: 2.7487940788269043\ncontent loss 1.309682846069336- style loss:1.4903431266866107e-10- pixel loss:0.3208561837673187\nEpoch 1/100 - D1 Loss: 0.5261034965515137 - G Loss: 2.7152414321899414\ncontent loss 1.2877286672592163- style loss:1.3996259706772207e-10- pixel loss:0.3259216248989105\nEpoch 1/100 - D1 Loss: 0.5036978125572205 - G Loss: 2.343482494354248\ncontent loss 1.247693419456482- style loss:1.3303615153947845e-10- pixel loss:0.25430595874786377\nEpoch 1/100 - D1 Loss: 0.5034353733062744 - G Loss: 3.1569366455078125\ncontent loss 1.2524513006210327- style loss:1.4567286266142787e-10- pixel loss:0.38977131247520447\nEpoch 1/100 - D1 Loss: 0.5035130381584167 - G Loss: 3.1117427349090576\ncontent loss 1.4161555767059326- style loss:1.667948834604971e-10- pixel loss:0.3795362710952759\nEpoch 1/100 - D1 Loss: 0.5039685368537903 - G Loss: 2.676159381866455\ncontent loss 1.1524689197540283- style loss:1.3285070266100263e-10- pixel loss:0.311449259519577\nEpoch 1: Avg. PSNR: 9.94\nEpoch 2/100 - D1 Loss: 0.5056496858596802 - G Loss: 2.742887496948242\ncontent loss 1.423473596572876- style loss:1.773956537221011e-10- pixel loss:0.3180104196071625\nEpoch 2/100 - D1 Loss: 0.503887414932251 - G Loss: 2.8840794563293457\ncontent loss 1.359153389930725- style loss:1.8805285106893166e-10- pixel loss:0.34252652525901794\nEpoch 2/100 - D1 Loss: 0.5034792423248291 - G Loss: 2.6433486938476562\ncontent loss 1.4074878692626953- style loss:1.9891965852281146e-10- pixel loss:0.3016139268875122\nEpoch 2/100 - D1 Loss: 0.5035200715065002 - G Loss: 3.0157270431518555\ncontent loss 1.481257438659668- style loss:1.8306377247423455e-10- pixel loss:0.3624367415904999\nEpoch 2/100 - D1 Loss: 0.5034868121147156 - G Loss: 2.9287922382354736\ncontent loss 1.1441084146499634- style loss:1.2881545830012442e-10- pixel loss:0.3535662293434143\nEpoch 2/100 - D1 Loss: 0.5033754706382751 - G Loss: 2.6594388484954834\ncontent loss 1.1832507848739624- style loss:1.2663428639037022e-10- pixel loss:0.3080223500728607\nEpoch 2/100 - D1 Loss: 0.5035527348518372 - G Loss: 2.6381547451019287\ncontent loss 1.289902925491333- style loss:1.6942364178262892e-10- pixel loss:0.3027523458003998\nEpoch 2/100 - D1 Loss: 0.5034118890762329 - G Loss: 2.8948276042938232\ncontent loss 1.456444263458252- style loss:1.7993667666971191e-10- pixel loss:0.34269940853118896\nEpoch 2/100 - D1 Loss: 0.5033519268035889 - G Loss: 3.083528518676758\ncontent loss 1.3580958843231201- style loss:1.5925868956934153e-10- pixel loss:0.3757842779159546\nEpoch 2/100 - D1 Loss: 0.5034189224243164 - G Loss: 2.857706308364868\ncontent loss 1.0614972114562988- style loss:1.0539188272096212e-10- pixel loss:0.3431137502193451\nEpoch 2/100 - D1 Loss: 0.5033395290374756 - G Loss: 2.7686266899108887\ncontent loss 1.2308764457702637- style loss:1.3871520598840448e-10- pixel loss:0.32541367411613464\nEpoch 2/100 - D1 Loss: 0.503422737121582 - G Loss: 2.3968632221221924\ncontent loss 1.4255646467208862- style loss:1.8479862085030163e-10- pixel loss:0.260236918926239\nEpoch 2/100 - D1 Loss: 0.5032967329025269 - G Loss: 2.9851577281951904\ncontent loss 1.3977574110031128- style loss:1.8877432950148432e-10- pixel loss:0.3587203919887543\nEpoch 2/100 - D1 Loss: 0.5032873153686523 - G Loss: 3.023919105529785\ncontent loss 1.262075424194336- style loss:1.6333139007951303e-10- pixel loss:0.36743661761283875\nEpoch 2/100 - D1 Loss: 0.5033186674118042 - G Loss: 2.635763645172119\ncontent loss 1.207993507385254- style loss:1.3388645747625105e-10- pixel loss:0.303650438785553\nEpoch 2/100 - D1 Loss: 0.5033016800880432 - G Loss: 2.230844020843506\ncontent loss 1.2585759162902832- style loss:1.420726175593856e-10- pixel loss:0.23532350361347198\nEpoch 2: Avg. PSNR: 9.92\nEpoch 3/100 - D1 Loss: 0.5034548044204712 - G Loss: 2.68243408203125\ncontent loss 1.2733373641967773- style loss:1.7183379719121206e-10- pixel loss:0.3103393614292145\n","output_type":"stream"}]}]}